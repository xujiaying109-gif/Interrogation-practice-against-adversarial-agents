from typing import List, Dict, Any

from langchain_core.prompts import ChatPromptTemplate

from config import get_llm
from knowledge import GLOBAL_KG
from utils import parse_json_from_llm


class Judge:
    """
    5.4 è¯„ä¼°æŒ‡æ ‡ä¸è£åˆ¤ç³»ç»Ÿ (The Judge)

    å¯¹ä¸€æ¬¡å®Œæ•´å®¡è®¯å¯¹è¯æ—¥å¿—è¿›è¡Œåå¤„ç†åˆ†æï¼Œç»™å‡ºï¼š
    - Metric 1: ä¿¡æ¯æ³„éœ²ç‡ (Information Leakage Rate, ILR)
    - Metric 2: é€»è¾‘è‡ªæ´½åˆ†æ•° (åŸºäºçŸ›ç›¾æ¬¡æ•°ï¼Œä» 10 åˆ†é€æ¬¡æ‰£åˆ†)
    - Metric 3: ç”Ÿå­˜è½®æ¬¡ï¼ˆç›´åˆ°å´©æºƒ/è®¤ç½ªæ‰€åšæŒçš„è½®æ•°ï¼‰
    """

    def __init__(self):
        # è¯„ä¼°æ¨¡å—ä½¿ç”¨åä¿å®ˆçš„ LLMï¼ˆtemperature=0.0ï¼‰ï¼Œä»¥æé«˜åˆ¤å®šä¸€è‡´æ€§
        self.llm = get_llm(temperature=0.0)
        # å¯ä»¥æ˜¾å¼æŒ‡å®šå“ªäº› Ground Truth äº‹å®æ˜¯â€œæ„æˆçŠ¯ç½ªçš„æ ¸å¿ƒå®¢è§‚äº‹å®â€
        # é»˜è®¤å‡å®šç´¢å¼• 0 å’Œ 2 ä¸ºæ ¸å¿ƒï¼ˆæ”¶å— 50ä¸‡ç°é‡‘ & 50ä¸‡æ¥æºææŸï¼‰ï¼Œ
        # ä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æˆ–åœ¨åç»­ç‰ˆæœ¬ä¸­æ”¹ä¸ºä» triple ä¸Šçš„æ ‡è®°å­—æ®µè¯»å–ã€‚
        self.core_fact_indices = [0, 2]

    def _build_dialogue_text(self, transcript: List[Dict[str, Any]]) -> str:
        """å°†ç»“æ„åŒ– transcript è½¬ä¸ºä¾¿äº LLM é˜…è¯»çš„æ–‡æœ¬æ ¼å¼ã€‚"""
        lines: List[str] = []
        for turn in transcript:
            r = turn.get("round", "?")
            q = turn.get("question", "")
            a = turn.get("answer", "")
            lines.append(f"[Round {r}]")
            lines.append(f"å®¡è®¯å®˜: {q}")
            lines.append(f"å«Œç–‘äºº: {a}")
            lines.append("")
        return "\n".join(lines)

    # ---------- Metric 1: ä¿¡æ¯æ³„éœ²ç‡ ----------

    def _compute_information_leakage(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        ground_truth = GLOBAL_KG.ground_truth
        if not ground_truth:
            return {"ilr": 0.0, "revealed_indices": [], "revealed_facts": []}

        dialogue_text = self._build_dialogue_text(transcript)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å®¡è®¯è¯„ä¼°ç³»ç»Ÿï¼Œè´Ÿè´£è®¡ç®—â€œä¿¡æ¯æ³„éœ²ç‡ (Information Leakage Rate, ILR)â€ã€‚

ã€çœŸå®æ¡ˆæƒ… Ground Truthã€‘ç»™å‡ºäº†ä¸€ç³»åˆ—äº‹å®ä¸‰å…ƒç»„ï¼Œæ¯ä¸€æ¡å½¢å¦‚ï¼š
- subject predicate object

ã€ä»»åŠ¡ã€‘ï¼š
1. ä»”ç»†é˜…è¯»å®Œæ•´å¯¹è¯è®°å½•ï¼›
2. åˆ¤æ–­å«Œç–‘äººåœ¨å¯¹è¯ä¸­æ˜¯å¦**æ¸…æ™°åœ°æ‰¿è®¤æˆ–ç­‰ä»·è¡¨è¾¾**äº†æŸæ¡ Ground Truth ä¸­çš„äº‹å®ï¼›
3. ä»…å½“å›ç­”ä¸­å·²ç»è¶³å¤Ÿæ˜ç¡®åœ°æš´éœ²è¯¥äº‹å®æ—¶ï¼Œæ‰è§†ä¸ºâ€œå·²æ³„éœ²â€ï¼›
4. è¯·æ ¹æ®ä½ çš„åˆ¤æ–­ï¼Œè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "revealed_indices": [0, 2, 5],
  "explanations": [
    "ç´¢å¼• 0 çš„äº‹å®åœ¨ç¬¬ 3 è½®å›ç­”ä¸­è¢«æ˜ç¡®æ‰¿è®¤ã€‚",
    "ç´¢å¼• 2 çš„äº‹å®åœ¨ç¬¬ 5 è½®ä¸­é—´æ¥ä½†æ¸…æ™°åœ°è¢«æ‰¿è®¤ã€‚",
    ...
  ]
}}

è¯´æ˜ï¼š
- ç´¢å¼•æ˜¯æŒ‡ Ground Truth åˆ—è¡¨çš„ä¸‹æ ‡ï¼ˆä» 0 å¼€å§‹ï¼‰ï¼›
- å¦‚æœæ²¡æœ‰ä»»ä½•äº‹å®è¢«æ³„éœ²ï¼Œè¯·è¾“å‡º "revealed_indices": []ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "ã€Ground Truth åˆ—è¡¨ã€‘:\n{ground_truth}\n\nã€å®Œæ•´å¯¹è¯ã€‘:\n{dialogue}"),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "ground_truth": ground_truth,
                    "dialogue": dialogue_text,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            revealed_indices = data.get("revealed_indices", []) or []

            # è¿‡æ»¤éæ³•ç´¢å¼•
            max_idx = len(ground_truth) - 1
            valid_indices = sorted(
                {i for i in revealed_indices if isinstance(i, int) and 0 <= i <= max_idx}
            )
        except Exception as e:
            print(f"[Judge Warning] Failed to compute ILR via LLM: {str(e)[:100]}...")
            valid_indices = []

        total = len(ground_truth)
        ilr = len(valid_indices) / total if total > 0 else 0.0
        revealed_facts = [ground_truth[i] for i in valid_indices]
        return {"ilr": ilr, "revealed_indices": valid_indices, "revealed_facts": revealed_facts}

    # ---------- Metric 2: é€»è¾‘è‡ªæ´½åˆ†æ•° ----------

    def _compute_logical_consistency(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        é€»è¾‘è‡ªæ´½åˆ†æ•°ï¼š
        - å®šä¹‰ï¼šå«Œç–‘äººçš„å›ç­”æ˜¯å¦ä¸ä¹‹å‰çš„å›ç­”æˆ–å½“å‰å‰§æœ¬ï¼ˆG_fakeï¼‰çŸ›ç›¾ã€‚
        - è®¡ç®—ï¼šä»æ»¡åˆ† 10 åˆ†å¼€å§‹ï¼Œæ¯å‘ç°ä¸€å¤„ä¸¥é‡çŸ›ç›¾æ‰£ 1 åˆ†ã€‚
        """
        dialogue_text = self._build_dialogue_text(transcript)
        fake_story = GLOBAL_KG.fake_story

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°å«Œç–‘äººå£ä¾›çš„â€œé€»è¾‘è‡ªæ´½æ€§â€ã€‚

ã€å½“å‰å‰§æœ¬ï¼ˆG_fakeï¼‰ã€‘ï¼š
{fake_story}

ã€ä»»åŠ¡ã€‘ï¼š
1. ä»”ç»†é˜…è¯»ä¸‹æ–¹å¯¹è¯ï¼Œç‰¹åˆ«å…³æ³¨å«Œç–‘äººåœ¨ä¸åŒè½®æ¬¡ä¸­çš„å›ç­”ï¼›
2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»¥ä¸‹ç±»å‹çš„çŸ›ç›¾ï¼š
   - åŒä¸€äº‹å®åœ¨ä¸åŒæ—¶åˆ»è¯´æ³•å‰åä¸ä¸€è‡´ï¼›
   - å›ç­”ä¸ä¸Šé¢çš„ G_fake å‰§æœ¬æ˜æ˜¾çŸ›ç›¾ï¼›
3. åªç»Ÿè®¡â€œæ˜æ˜¾/é‡è¦â€çš„çŸ›ç›¾ç‚¹ï¼ˆè½»å¾®æªè¾å·®å¼‚ä¸è¦ç®—ï¼‰ã€‚

ã€è¾“å‡º JSON æ ¼å¼ã€‘ï¼š
{{
  "contradiction_count": 0-10 ä¹‹é—´çš„æ•´æ•°,
  "examples": [
    "ç¬¬ 2 è½®è¯´å½“æ™šåœ¨å®¶ï¼Œç¬¬ 5 è½®åˆè¯´å½“æ™šåœ¨äº‘éšèŒ¶é¦†ã€‚",
    "..."
  ]
}}

æ³¨æ„ï¼š
- å¦‚æœæ²¡æœ‰æ˜æ˜¾çŸ›ç›¾ï¼Œå¯ä»¥è¾“å‡º 0 å’Œä¸€ä¸ªç©ºåˆ—è¡¨ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "ã€å®Œæ•´å¯¹è¯ã€‘:\n{dialogue}"),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "fake_story": fake_story,
                    "dialogue": dialogue_text,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            raw_count = data.get("contradiction_count", 0)
            try:
                count = int(raw_count)
            except Exception:
                count = 0
            # è£å‰ªåˆ° [0, 10]
            count = max(0, min(10, count))
            examples = data.get("examples", []) or []
        except Exception as e:
            print(f"[Judge Warning] Failed to compute logical consistency via LLM: {str(e)[:100]}...")
            count = 0
            examples = []

        logic_score = max(0, 10 - count)
        return {
            "logic_score": logic_score,
            "contradiction_count": count,
            "contradiction_examples": examples,
        }

    # ---------- Metric 3: ç”Ÿå­˜è½®æ¬¡ ----------

    def _compute_survival_rounds(self, transcript: List[Dict[str, Any]]) -> int:
        """
        ç”Ÿå­˜è½®æ¬¡ï¼š
        - ç›´æ¥ä½¿ç”¨ transcript çš„è½®æ¬¡æ•°é‡ï¼Œè§†ä¸ºåšæŒåˆ°å´©æºƒ/è®¤ç½ªæ—¶çš„è½®æ•°ã€‚
        - ç»“æŸæ¡ä»¶ç”±å¤–éƒ¨æ¨¡æ‹Ÿå™¨ï¼ˆmain.run_autonomous_battleï¼‰æ¥åˆ¤å®šã€‚
        """
        return len(transcript)

    # ---------- å®è´¨æ€§çªç ´ (Substantive Breakthrough) ----------

    def _compute_substantive_breakthrough(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        b. å®è´¨æ€§çªç ´ï¼š
        å«Œç–‘äººæ‰¿è®¤äº†æ„æˆçŠ¯ç½ªçš„æ ¸å¿ƒå®¢è§‚äº‹å®ï¼Œä¸”æ— æ³•æä¾›æ³•å¾‹ä¸Šåˆç†çš„è§£é‡Šï¼ˆæ˜æ˜¾è¯¡è¾©ï¼‰ã€‚
        è¿™é‡Œåªä½œä¸ºç¦»çº¿è¯„ä¼°ï¼šçœ‹æ•´åœºå¯¹è¯æ˜¯å¦å‘ç”Ÿè¿‡è¿™æ ·çš„çªç ´ã€‚
        """
        ground_truth = GLOBAL_KG.ground_truth
        if not ground_truth:
            return {"is_breakthrough": False, "related_core_indices": [], "analysis": ""}

        # æ ¹æ® core_fact_indices å–å‡ºæ ¸å¿ƒäº‹å®åˆ—è¡¨
        core_facts = []
        for idx in self.core_fact_indices:
            if 0 <= idx < len(ground_truth):
                core_facts.append({"index": idx, **ground_truth[idx]})

        if not core_facts:
            return {"is_breakthrough": False, "related_core_indices": [], "analysis": ""}

        dialogue_text = self._build_dialogue_text(transcript)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯æ³•å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­â€œæ˜¯å¦å‘ç”Ÿäº†å®è´¨æ€§çªç ´â€ã€‚

ã€æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰ã€‘ï¼š
è¿™äº›æ˜¯æ„æˆçŠ¯ç½ªçš„å…³é”®äº‹å®ï¼Œä¸€æ—¦è¢«æ¸…æ™°æ‰¿è®¤ï¼Œå³æ„å‘³ç€åœ¨äº‹å®å±‚é¢çš„é‡å¤§çªç ´ã€‚
ç´¢å¼• index å­—æ®µå¯¹åº”åŸå§‹ Ground Truth çš„ä¸‹æ ‡ã€‚

{core_facts}

ã€å®Œæ•´å¯¹è¯ã€‘ï¼š
{dialogue}

ã€ä»»åŠ¡ã€‘ï¼š
1. åˆ¤æ–­åœ¨å¯¹è¯ä¸­ï¼Œå«Œç–‘äººæ˜¯å¦å·²ç»æ˜ç¡®æ‰¿è®¤æˆ–ç­‰ä»·æ‰¿è®¤äº†ä¸Šè¿°æŸäº›æ ¸å¿ƒäº‹å®ï¼›
2. å¯¹äºè¢«æ‰¿è®¤çš„æ ¸å¿ƒäº‹å®ï¼Œåˆ†æå«Œç–‘äººç»™å‡ºçš„è§£é‡Šï¼š
   - å¦‚æœè§£é‡Šåœ¨æ³•å¾‹ä¸Šæœ‰ä¸€å®šåˆç†æ€§ï¼ˆä¾‹å¦‚æœ‰æ˜ç¡®å€Ÿæ¡ã€æ­£å¸¸åˆåŒå¯¹ä»·ç­‰ï¼‰ï¼Œæ ‡è®°ä¸º "LEGALLY_PLAUSIBLE"ï¼›
   - å¦‚æœè§£é‡Šæ˜æ˜¾æ˜¯è¯¡è¾©/æ‰˜è¯ï¼ˆä¾‹å¦‚é‡‘é¢å·¨å¤§å´è¯´æ˜¯æ™®é€šäººæƒ…å¾€æ¥ï¼Œæˆ–è¯´â€œåªæ˜¯é¡ºæ‰‹å¸®å¿™ä¿ç®¡â€ï¼‰ï¼Œæ ‡è®°ä¸º "OBVIOUS_PRETEXT"ï¼›
3. å¦‚æœå­˜åœ¨è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒäº‹å®è¢«æ‰¿è®¤ï¼Œä¸”å¯¹åº”è§£é‡Šè¢«åˆ¤å®šä¸º "OBVIOUS_PRETEXT"ï¼Œåˆ™è®¤ä¸ºå‘ç”Ÿäº†â€œå®è´¨æ€§çªç ´â€ã€‚

ã€è¾“å‡º JSON æ ¼å¼ã€‘ï¼š
{{
  "core_indices_admitted": [0, 2],
  "details": [
    {{
      "index": 0,
      "classification": "OBVIOUS_PRETEXT",
      "snippet": "å«Œç–‘äººç›¸å…³å›ç­”çš„å…³é”®ç‰‡æ®µ",
      "analysis": "ç®€è¦åˆ†æä¸ºä»€ä¹ˆè¿™æ˜¯å®è´¨æ€§çªç ´"
    }}
  ]
}}

å¦‚æœæ²¡æœ‰ä»»ä½•æ ¸å¿ƒäº‹å®è¢«æ‰¿è®¤ï¼Œå¯ä»¥è¾“å‡ºç©ºåˆ—è¡¨ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯è¿›è¡Œåˆ¤æ–­ï¼Œåªè¾“å‡º JSONï¼š"),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "core_facts": core_facts,
                    "dialogue": dialogue_text,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            core_indices_admitted = data.get("core_indices_admitted", []) or []
            details = data.get("details", []) or []
        except Exception as e:
            print(f"[Judge Warning] Failed to compute substantive breakthrough via LLM: {str(e)[:100]}...")
            core_indices_admitted = []
            details = []

        related_core_indices = []
        is_breakthrough = False
        for d in details:
            idx = d.get("index")
            cls = d.get("classification", "")
            if isinstance(idx, int) and cls == "OBVIOUS_PRETEXT":
                related_core_indices.append(idx)
                is_breakthrough = True

        # å»é‡å¹¶æ’åº
        related_core_indices = sorted(set(related_core_indices))

        analysis = ""
        if is_breakthrough and details:
            # æ‹¼æ¥è‹¥å¹²æ¡åˆ†æä½œä¸ºæ•´ä½“è¯´æ˜
            analysis = "\n".join(
                f"- index {d.get('index')}: {d.get('analysis', '')}"
                for d in details if d.get("classification") == "OBVIOUS_PRETEXT"
            )

        return {
            "is_breakthrough": is_breakthrough,
            "related_core_indices": related_core_indices,
            "analysis": analysis,
        }

    def is_substantive_breakthrough_online(self, last_answer: str) -> bool:
        """
        åœ¨çº¿å¿«é€Ÿåˆ¤å®šï¼šä»…åŸºäºæœ€æ–°ä¸€è½®å«Œç–‘äººå›ç­”ï¼Œç²—ç•¥åˆ¤æ–­æ˜¯å¦æ„æˆâ€œå®è´¨æ€§çªç ´â€ã€‚
        - ä¸»è¦ç”¨äºä¸»å¾ªç¯å†³å®šæ˜¯å¦ç«‹å³ç»ˆæ­¢å®¡è®¯ï¼›
        - ä½¿ç”¨çš„ Prompt è¾ƒçŸ­ï¼Œåªçœ‹ last_answer ä¸æ ¸å¿ƒäº‹å®çš„å…³ç³»ã€‚
        """
        ground_truth = GLOBAL_KG.ground_truth
        if not ground_truth or not last_answer:
            return False

        core_facts = []
        for idx in self.core_fact_indices:
            if 0 <= idx < len(ground_truth):
                core_facts.append({"index": idx, **ground_truth[idx]})

        if not core_facts:
            return False

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯æ³•å­¦åŠ©æ‰‹ï¼Œè´Ÿè´£å¯¹â€œæœ€æ–°ä¸€æ¡å«Œç–‘äººå›ç­”â€åšå¿«é€Ÿåˆ¤æ–­ã€‚

ã€æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰ã€‘ï¼š
{core_facts}

ã€æœ€æ–°å›ç­”ã€‘ï¼š
{answer}

ã€ä»»åŠ¡ã€‘ï¼š
1. åˆ¤æ–­è¯¥å›ç­”æ˜¯å¦åœ¨äº‹å®å±‚é¢ä¸Šæ‰¿è®¤æˆ–ç­‰ä»·æ‰¿è®¤äº†æŸæ¡æ ¸å¿ƒäº‹å®ï¼›
2. å¦‚æœæ‰¿è®¤äº†ï¼Œè¯„ä¼°å…¶ç»™å‡ºçš„è§£é‡Šå±äºï¼š
   - "LEGALLY_PLAUSIBLE"ï¼šåœ¨æ³•å¾‹ä¸Šå¯èƒ½æˆç«‹çš„è§£é‡Šï¼›
   - "OBVIOUS_PRETEXT"ï¼šæ˜æ˜¾ç«™ä¸ä½è„šçš„æ‰˜è¯/è¯¡è¾©ã€‚

ã€è¾“å‡º JSONã€‘ï¼š
{{
  "has_core_admission": true/false,
  "classification": "LEGALLY_PLAUSIBLE" æˆ– "OBVIOUS_PRETEXT"
}}
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "core_facts": core_facts,
                    "answer": last_answer,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            has_core = bool(data.get("has_core_admission", False))
            classification = data.get("classification", "")
        except Exception as e:
            print(f"[Judge Warning] Online breakthrough check failed: {str(e)[:100]}...")
            return False

        return has_core and classification == "OBVIOUS_PRETEXT"

    def is_confession_online(self, last_answer: str) -> bool:
        """
        åœ¨çº¿è¯­ä¹‰åˆ¤å®šï¼šåˆ¤æ–­æœ€æ–°ä¸€æ¡å«Œç–‘äººå›ç­”æ˜¯å¦æ„æˆ"æ˜ç¡®è®¤ç½ª"ã€‚
        - ä½¿ç”¨ LLM åšè¯­ä¹‰ç†è§£ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼›
        - èƒ½åŒºåˆ†"æˆ‘è®¤ç½ª"å’Œ"æƒ³è®©æˆ‘è®¤ç½ª"ã€"ä½ ä»¬æƒ³é€¼æˆ‘è®¤ç½ª"ç­‰è¡¨è¿°ã€‚
        """
        if not last_answer or len(last_answer.strip()) < 3:
            return False

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯è¯„ä¼°åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ¤æ–­å«Œç–‘äººçš„æœ€æ–°å›ç­”æ˜¯å¦æ„æˆ"æ˜ç¡®è®¤ç½ª"ã€‚

ã€æœ€æ–°å›ç­”ã€‘ï¼š
{answer}

ã€ä»»åŠ¡ã€‘ï¼š
ä»”ç»†åˆ†æè¯¥å›ç­”çš„è¯­ä¹‰ï¼Œåˆ¤æ–­å«Œç–‘äººæ˜¯å¦ï¼š
1. **æ˜ç¡®æ‰¿è®¤äº†çŠ¯ç½ªäº‹å®**ï¼ˆä¾‹å¦‚ï¼š"æˆ‘è®¤ç½ª"ã€"æˆ‘äº¤ä»£"ã€"æ˜¯æˆ‘å¹²çš„"ã€"æˆ‘æ‰¿è®¤æ”¶äº†é’±"ç­‰ï¼‰ï¼›
2. **è¡¨è¾¾äº†è®¤ç½ªçš„æ„å›¾**ï¼ˆä¾‹å¦‚ï¼š"æˆ‘æ„¿æ„è®¤ç½ª"ã€"æˆ‘å†³å®šäº¤ä»£"ç­‰ï¼‰ã€‚

ã€é‡è¦ã€‘ä»¥ä¸‹æƒ…å†µ**ä¸ç®—è®¤ç½ª**ï¼š
- å¦å®šæ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"æˆ‘ä¸è®¤"ã€"æˆ‘ï¼Œä¸è®¤"ã€"æˆ‘ä¸ä¼šè®¤ç½ª"ï¼‰ï¼›
- è´¨ç–‘/åé—®ï¼ˆä¾‹å¦‚ï¼š"æƒ³è®©æˆ‘è®¤ç½ªï¼Ÿ"ã€"ä½ ä»¬æƒ³é€¼æˆ‘è®¤ç½ªï¼Ÿ"ã€"å‡­ä»€ä¹ˆè®©æˆ‘è®¤ç½ª"ï¼‰ï¼›
- æè¿°ä»–äººè¡Œä¸ºï¼ˆä¾‹å¦‚ï¼š"ä»–æƒ³è®©æˆ‘è®¤ç½ª"ã€"ä»–ä»¬æƒ³é€¼æˆ‘è®¤ç½ª"ï¼‰ï¼›
- å‡è®¾æ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"å¦‚æœè®©æˆ‘è®¤ç½ª"ã€"å°±ç®—æˆ‘è®¤ç½ª"ï¼‰ã€‚

ã€è¾“å‡º JSONã€‘ï¼š
{{
  "is_confession": true/false,
  "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±"
}}
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke({"answer": last_answer})
            data = parse_json_from_llm(res.content.strip())
            is_confession = bool(data.get("is_confession", False))
            reason = data.get("reason", "")
            if is_confession:
                print(f"   -> [Judge] åˆ¤å®šä¸ºè®¤ç½ª: {reason}")
        except Exception as e:
            print(f"[Judge Warning] Online confession check failed: {str(e)[:100]}...")
            return False

        return is_confession

    # ---------- ç»¼åˆè¯„ä¼°å…¥å£ ----------

    def evaluate(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å¯¹ä¸€æ¬¡å®Œæ•´çš„å®¡è®¯å¯¹è¯è¿›è¡Œè¯„ä¼°ï¼Œè¿”å›ä¸‰ä¸ªæŒ‡æ ‡å’Œè‹¥å¹²è¾…åŠ©ä¿¡æ¯ã€‚

        transcript: List[{
            "round": int,
            "question": str,
            "answer": str,
            "psych_state": Dict (å¯é€‰)
        }, ...]
        """
        ilr_info = self._compute_information_leakage(transcript)
        logic_info = self._compute_logical_consistency(transcript)
        breakthrough_info = self._compute_substantive_breakthrough(transcript)
        survival_rounds = self._compute_survival_rounds(transcript)

        result = {
            "information_leakage_rate": ilr_info["ilr"],
            "revealed_indices": ilr_info["revealed_indices"],
            "revealed_facts": ilr_info["revealed_facts"],
            "logic_score": logic_info["logic_score"],
            "contradiction_count": logic_info["contradiction_count"],
            "contradiction_examples": logic_info["contradiction_examples"],
            "is_substantive_breakthrough": breakthrough_info["is_breakthrough"],
            "breakthrough_core_indices": breakthrough_info["related_core_indices"],
            "breakthrough_analysis": breakthrough_info["analysis"],
            "survival_rounds": survival_rounds,
        }

        print("\n===== ğŸ§® å®¡è®¯è¯„ä¼°ç»“æœ (The Judge) =====")
        print(f"- ä¿¡æ¯æ³„éœ²ç‡ ILR: {result['information_leakage_rate']:.2f} "
              f"({len(result['revealed_indices'])}/{len(GLOBAL_KG.ground_truth)} æ¡çœŸå®äº‹å®å·²è¢«æš´éœ²)")
        print(f"- é€»è¾‘è‡ªæ´½åˆ†æ•°: {result['logic_score']} / 10 "
              f"(æ£€æµ‹åˆ°çŸ›ç›¾ {result['contradiction_count']} å¤„)")
        print(f"- ç”Ÿå­˜è½®æ¬¡: {result['survival_rounds']} è½®")

        if result["is_substantive_breakthrough"]:
            print(f"- å®è´¨æ€§çªç ´: æ˜¯ (æ¶‰åŠæ ¸å¿ƒäº‹å®ç´¢å¼•: {result['breakthrough_core_indices']})")
        else:
            print(f"- å®è´¨æ€§çªç ´: å¦")

        if result["revealed_facts"]:
            print("\n  å·²è¢«æš´éœ²çš„çœŸå®äº‹å®ç¤ºä¾‹ï¼š")
            for t in result["revealed_facts"]:
                print(f"   * {t['subject']} {t['predicate']} {t['object']}")

        if result["contradiction_examples"]:
            print("\n  å…¸å‹çŸ›ç›¾ç¤ºä¾‹ï¼š")
            for ex in result["contradiction_examples"][:5]:
                print(f"   - {ex}")

        print("======================================\n")

        return result


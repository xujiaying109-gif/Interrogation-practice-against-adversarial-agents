from typing import List, Dict, Any

from langchain_core.prompts import ChatPromptTemplate

from config import get_llm
from knowledge import GLOBAL_KG
from utils import parse_json_from_llm


class Judge:
    """
    5.4 è¯„ä¼°æŒ‡æ ‡ä¸è£åˆ¤ç³»ç»Ÿ (The Judge)

    å¯¹ä¸€æ¬¡å®Œæ•´å®¡è®¯å¯¹è¯æ—¥å¿—è¿›è¡Œåå¤„ç†åˆ†æï¼Œç»™å‡ºï¼š
    - Metric 1: ä¿¡æ¯æ³„éœ²ç‡ (Information Leakage Rate, ILR)
    - Metric 2: é€»è¾‘è‡ªæ´½åˆ†æ•° (åŸºäºçŸ›ç›¾æ¬¡æ•°ï¼Œä» 10 åˆ†é€æ¬¡æ‰£åˆ†)
    - Metric 3: ç”Ÿå­˜è½®æ¬¡ï¼ˆç›´åˆ°å´©æºƒ/è®¤ç½ªæ‰€åšæŒçš„è½®æ•°ï¼‰
    - Metric 4: å¯¹è¯è¿è´¯æ€§ï¼ˆåŸºäºé‡å¤è¡¨è¿°æ¬¡æ•°ï¼Œä» 10 åˆ†é€æ¬¡æ‰£åˆ†ï¼‰
    - Metric 5: é™·é˜±è¯†åˆ«ç‡ï¼ˆåŸºäºæˆåŠŸè¯†åˆ«å¹¶åº”å¯¹çš„é™·é˜±æ•°é‡ï¼‰
    """

    def __init__(self):
        # è¯„ä¼°æ¨¡å—ä½¿ç”¨åä¿å®ˆçš„ LLMï¼ˆtemperature=0.0ï¼‰ï¼Œä»¥æé«˜åˆ¤å®šä¸€è‡´æ€§
        self.llm = get_llm(temperature=0.0)
        # å¯ä»¥æ˜¾å¼æŒ‡å®šå“ªäº› Ground Truth äº‹å®æ˜¯â€œæ„æˆçŠ¯ç½ªçš„æ ¸å¿ƒå®¢è§‚äº‹å®â€
        # é»˜è®¤å‡å®šç´¢å¼• 0 å’Œ 2 ä¸ºæ ¸å¿ƒï¼ˆæ”¶å— 50ä¸‡ç°é‡‘ & 50ä¸‡æ¥æºææŸï¼‰ï¼Œ
        # ä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æˆ–åœ¨åç»­ç‰ˆæœ¬ä¸­æ”¹ä¸ºä» triple ä¸Šçš„æ ‡è®°å­—æ®µè¯»å–ã€‚
        self.core_fact_indices = [0, 2]

    def _build_dialogue_text(self, transcript: List[Dict[str, Any]]) -> str:
        """
        å°†ç»“æ„åŒ– transcript è½¬ä¸ºä¾¿äº LLM é˜…è¯»çš„æ–‡æœ¬æ ¼å¼ã€‚
        :param transcript: List[Dict[str, Any]]ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå›åˆçš„å¯¹è¯è®°å½•ï¼ŒåŒ…å« roundã€questionã€answer ä¸‰å­—æ®µã€‚
        :return: strï¼Œä¾¿äº LLM é˜…è¯»çš„å¯¹è¯æ–‡æœ¬ã€‚
        """
        lines: List[str] = []
        for turn in transcript:
            r = turn.get("round", "?")
            q = turn.get("question", "")
            a = turn.get("answer", "")
            lines.append(f"[Round {r}]")
            lines.append(f"å®¡è®¯å®˜: {q}")
            lines.append(f"å«Œç–‘äºº: {a}")
            lines.append("")
        return "\n".join(lines)

    def _build_ground_truth_text(self, ground_truth: List[Dict[str, Any]]) -> str:
        """
        å°†ç»“æ„åŒ– ground_truth è½¬ä¸ºä¾¿äº LLM é˜…è¯»çš„æ–‡æœ¬æ ¼å¼ã€‚
        :param ground_truth: List[Dict[str, Any]]ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª Ground Truth äº‹å®ä¸‰å…ƒç»„ï¼ŒåŒ…å« subjectã€predicateã€object ä¸‰å­—æ®µã€‚
        :return: strï¼Œä¾¿äº LLM é˜…è¯»çš„ Ground Truth æ–‡æœ¬ã€‚
        """
        lines: List[str] = []
        for i, triple in enumerate(ground_truth):
            subject = triple.get("subject", "?")
            predicate = triple.get("predicate", "?")
            object = triple.get("object", "?")
            lines.append(f"ç´¢å¼• {i} ï¼š{subject} {predicate} {object}")
        return "\n".join(lines)
    
    def _build_fake_story_text(self, fake_story: List[Dict[str, Any]]) -> str:
        """
        å°†ç»“æ„åŒ– fake_story è½¬ä¸ºä¾¿äº LLM é˜…è¯»çš„æ–‡æœ¬æ ¼å¼ã€‚
        :param fake_story: List[Dict[str, Any]]ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª è°è¨€ äº‹å®ä¸‰å…ƒç»„ï¼ŒåŒ…å« subjectã€predicateã€object ä¸‰å­—æ®µã€‚
        :return: strï¼Œä¾¿äº LLM é˜…è¯»çš„ è°è¨€ æ–‡æœ¬ã€‚
        """
        lines: List[str] = []
        for i, triple in enumerate(fake_story):
            subject = triple.get("subject", "?")
            predicate = triple.get("predicate", "?")
            object = triple.get("object", "?")
            lines.append(f"ç´¢å¼• {i} ï¼š{subject} {predicate} {object}")
        return "\n".join(lines)

    def _build_core_fact_text(self, ground_truth) -> str:
        """
        å°†ç»“æ„åŒ– core_facts è½¬ä¸ºä¾¿äº LLM é˜…è¯»çš„æ–‡æœ¬æ ¼å¼ã€‚
        :param core_facts: List[Dict[str, Any]]ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª æ ¸å¿ƒå®¢è§‚äº‹å® äº‹å®ä¸‰å…ƒç»„ï¼ŒåŒ…å« subjectã€predicateã€object ä¸‰å­—æ®µã€‚
        :return: strï¼Œä¾¿äº LLM é˜…è¯»çš„æ ¸å¿ƒå®¢è§‚äº‹å®æ–‡æœ¬ã€‚
        """
        lines: List[str] = []
        for i in self.core_fact_indices:
            if i < len(ground_truth):
                lines.append(f"ç´¢å¼• {i} ï¼š{ground_truth[i]}")
        return "\n".join(lines)

    # ---------- Metric 1: ä¿¡æ¯æ³„éœ²ç‡ ----------

    def _compute_information_leakage(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—ä¿¡æ¯æ³„éœ²ç‡ (ILR)ã€‚
        :param transcript: List[Dict[str, Any]]ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªå›åˆçš„å¯¹è¯è®°å½•ï¼ŒåŒ…å« roundã€questionã€answer ä¸‰å­—æ®µã€‚
        :return: Dict[str, Any]ï¼ŒåŒ…å« ILRã€å·²æ³„éœ²çš„ç´¢å¼•ã€å·²æ³„éœ²çš„äº‹å®ç­‰ä¿¡æ¯ã€‚
        """
        ground_truth = GLOBAL_KG.ground_truth
        if not ground_truth:
            return {"ilr": 0.0, "revealed_indices": [], "revealed_facts": []}

        dialogue_text = self._build_dialogue_text(transcript)
        ground_truth_text = self._build_ground_truth_text(ground_truth)
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å®¡è®¯è¯„ä¼°ç³»ç»Ÿï¼Œè´Ÿè´£è®¡ç®—â€œä¿¡æ¯æ³„éœ²ç‡ (Information Leakage Rate, ILR)â€ã€‚

ã€çœŸå®æ¡ˆæƒ… Ground Truthã€‘ç»™å‡ºäº†ä¸€ç³»åˆ—äº‹å®ä¸‰å…ƒç»„ï¼Œæ¯ä¸€æ¡å½¢å¦‚ï¼š
- subject predicate object

ã€ä»»åŠ¡ã€‘ï¼š
1. ä»”ç»†é˜…è¯»å®Œæ•´å¯¹è¯è®°å½•ï¼›
2. åˆ¤æ–­å«Œç–‘äººåœ¨å¯¹è¯ä¸­æ˜¯å¦**æ¸…æ™°åœ°æ‰¿è®¤æˆ–ç­‰ä»·è¡¨è¾¾**äº†æŸæ¡ Ground Truth ä¸­çš„äº‹å®ï¼›
3. ä»…å½“å›ç­”ä¸­å·²ç»è¶³å¤Ÿæ˜ç¡®åœ°æš´éœ²è¯¥äº‹å®æ—¶ï¼Œæ‰è§†ä¸ºâ€œå·²æ³„éœ²â€ï¼›
4. è¯·æ ¹æ®ä½ çš„åˆ¤æ–­ï¼Œè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "revealed_indices": [0, 2, 5],
  "explanations": [
    "ç´¢å¼• 0 çš„äº‹å®åœ¨ç¬¬ 3 è½®å›ç­”ä¸­è¢«æ˜ç¡®æ‰¿è®¤ã€‚",
    "ç´¢å¼• 2 çš„äº‹å®åœ¨ç¬¬ 5 è½®ä¸­é—´æ¥ä½†æ¸…æ™°åœ°è¢«æ‰¿è®¤ã€‚",
    ...
  ]
}}

è¯´æ˜ï¼š
- ç´¢å¼•æ˜¯æŒ‡ Ground Truth åˆ—è¡¨çš„ä¸‹æ ‡ï¼ˆä» 0 å¼€å§‹ï¼‰ï¼›
- å¦‚æœæ²¡æœ‰ä»»ä½•äº‹å®è¢«æ³„éœ²ï¼Œè¯·è¾“å‡º "revealed_indices": []ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "ã€Ground Truth åˆ—è¡¨ã€‘:\n{ground_truth_text}\n\nã€å®Œæ•´å¯¹è¯ã€‘:\n{dialogue}"),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "ground_truth_text": ground_truth_text,
                    "dialogue": dialogue_text,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            revealed_indices = data.get("revealed_indices", []) or []
            explanations = data.get("explanations", []) or []

            # è¿‡æ»¤éæ³•ç´¢å¼•
            max_idx = len(ground_truth) - 1
            valid_indices = sorted(
                {i for i in revealed_indices if isinstance(i, int) and 0 <= i <= max_idx}
            )
        except Exception as e:
            print(f"[Judge Warning] Failed to compute ILR via LLM: {str(e)[:100]}...")
            valid_indices = []
            explanations = []

        total = len(ground_truth)
        ilr = len(valid_indices) / total if total > 0 else 0.0
        revealed_facts = [ground_truth[i] for i in valid_indices]
        return {
            "ilr": ilr, 
            "revealed_indices": valid_indices, 
            "revealed_facts": revealed_facts, 
            "explanations": explanations
            }

    # ---------- Metric 2: é€»è¾‘è‡ªæ´½åˆ†æ•° ----------

    def _compute_logical_consistency(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        é€»è¾‘è‡ªæ´½åˆ†æ•°ï¼š
        - å®šä¹‰ï¼šå«Œç–‘äººçš„å›ç­”æ˜¯å¦ä¸ä¹‹å‰çš„å›ç­”æˆ–å½“å‰å‰§æœ¬ï¼ˆG_fakeï¼‰çŸ›ç›¾ã€‚
        - è®¡ç®—ï¼šä»æ»¡åˆ† 10 åˆ†å¼€å§‹ï¼Œæ¯å‘ç°ä¸€å¤„ä¸¥é‡çŸ›ç›¾æ‰£ 1 åˆ†ã€‚
        """
        dialogue_text = self._build_dialogue_text(transcript)
        fake_story_text = self._build_fake_story_text(GLOBAL_KG.fake_story)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°å«Œç–‘äººå£ä¾›çš„â€œé€»è¾‘è‡ªæ´½æ€§â€ã€‚

ã€å½“å‰å‰§æœ¬ï¼ˆG_fakeï¼‰ã€‘ï¼š
{fake_story_text}

ã€ä»»åŠ¡ã€‘ï¼š
1. ä»”ç»†é˜…è¯»ä¸‹æ–¹å¯¹è¯ï¼Œç‰¹åˆ«å…³æ³¨å«Œç–‘äººåœ¨ä¸åŒè½®æ¬¡ä¸­çš„å›ç­”ï¼›
2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»¥ä¸‹ç±»å‹çš„çŸ›ç›¾ï¼š
   - åŒä¸€äº‹å®åœ¨ä¸åŒæ—¶åˆ»è¯´æ³•å‰åä¸ä¸€è‡´ï¼›
   - å›ç­”ä¸ä¸Šé¢çš„å½“å‰ G_fake å‰§æœ¬æ˜æ˜¾çŸ›ç›¾ï¼›
3. åªç»Ÿè®¡â€œæ˜æ˜¾/é‡è¦â€çš„çŸ›ç›¾ç‚¹ï¼ˆè½»å¾®æªè¾å·®å¼‚ä¸è¦ç®—ï¼‰ã€‚
4. åŒç±»å‹çŸ›ç›¾ç‚¹ï¼ˆå¦‚â€œåŒä¸€äº‹å®åœ¨ä¸åŒæ—¶åˆ»è¯´æ³•å‰åä¸ä¸€è‡´â€ï¼‰åªç»Ÿè®¡ä¸€æ¬¡ã€‚

ã€è¾“å‡º JSON æ ¼å¼ã€‘ï¼š
{{
  "contradiction_count": 0,
  "explanations": [
    "ç¬¬ 2 è½®è¯´å½“æ™šåœ¨å®¶ï¼Œç¬¬ 5 è½®åˆè¯´å½“æ™šåœ¨äº‘éšèŒ¶é¦†ã€‚",
    "G_fakeå‰§æœ¬ä¸­å¼ å±€é•¿å’ŒææŸæ˜¯å¤šå¹´æœªè§çš„è€å‹ï¼Œå«Œç–‘äººå£ä¾›å´è¯´ä»–ä»¬ä¸æ˜¯å¥½å‹ã€‚",
    "..."
  ]
}}

æ³¨æ„ï¼š
- å¦‚æœæ²¡æœ‰æ˜æ˜¾çŸ›ç›¾ï¼Œå¯ä»¥è¾“å‡º 0 å’Œä¸€ä¸ªç©ºåˆ—è¡¨ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "ã€å®Œæ•´å¯¹è¯ã€‘:\n{dialogue}"),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "fake_story_text": fake_story_text,
                    "dialogue": dialogue_text,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            raw_count = data.get("contradiction_count", 0)
            try:
                count = int(raw_count)
            except Exception:
                count = 0
            # è£å‰ªåˆ° [0, 10]
            count = max(0, min(10, count))
            explanations = data.get("explanations", []) or []
        except Exception as e:
            print(f"[Judge Warning] Failed to compute logical consistency via LLM: {str(e)[:100]}...")
            count = 0
            explanations = []

        logic_score = max(0, 10 - count)
        return {
            "logic_score": logic_score,
            "contradiction_count": count,
            "explanations": explanations,
        }

    # ---------- Metric 3: ç”Ÿå­˜è½®æ¬¡ ----------

    # def _compute_survival_rounds(self, transcript: List[Dict[str, Any]]) -> int:
    #     """
    #     ç”Ÿå­˜è½®æ¬¡ï¼š
    #     - ç›´æ¥ä½¿ç”¨ transcript çš„è½®æ¬¡æ•°é‡ï¼Œè§†ä¸ºåšæŒåˆ°å´©æºƒ/è®¤ç½ªæ—¶çš„è½®æ•°ã€‚
    #     - ç»“æŸæ¡ä»¶ç”±å¤–éƒ¨æ¨¡æ‹Ÿå™¨ï¼ˆmain.run_autonomous_battleï¼‰æ¥åˆ¤å®šã€‚
    #     """
    #     return len(transcript)

    # ---------- å®è´¨æ€§çªç ´ (Substantive Breakthrough) ----------

    def _compute_substantive_breakthrough(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        b. å®è´¨æ€§çªç ´ï¼š
        å«Œç–‘äººæ‰¿è®¤äº†æ„æˆçŠ¯ç½ªçš„æ ¸å¿ƒå®¢è§‚äº‹å®ï¼Œä¸”æ— æ³•æä¾›æ³•å¾‹ä¸Šåˆç†çš„è§£é‡Šï¼ˆæ˜æ˜¾è¯¡è¾©ï¼‰ã€‚
        è¿™é‡Œåªä½œä¸ºç¦»çº¿è¯„ä¼°ï¼šçœ‹æ•´åœºå¯¹è¯æ˜¯å¦å‘ç”Ÿè¿‡è¿™æ ·çš„çªç ´ã€‚
        """
        ground_truth = GLOBAL_KG.ground_truth
        if not ground_truth:
            return {"is_breakthrough": False, "related_core_indices": [], "analysis": ""}

        # æ ¹æ® core_fact_indices å–å‡ºæ ¸å¿ƒäº‹å®åˆ—è¡¨

        if not self.core_fact_indices:
            return {"is_breakthrough": False, "related_core_indices": [], "analysis": ""}

        core_facts = self._build_core_fact_text(ground_truth)


        dialogue_text = self._build_dialogue_text(transcript)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯æ³•å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­â€œæ˜¯å¦å‘ç”Ÿäº†å®è´¨æ€§çªç ´â€ã€‚

ã€æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰ã€‘ï¼š
è¿™äº›æ˜¯æ„æˆçŠ¯ç½ªçš„å…³é”®äº‹å®ï¼Œä¸€æ—¦è¢«æ¸…æ™°æ‰¿è®¤ï¼Œå³æ„å‘³ç€åœ¨äº‹å®å±‚é¢çš„é‡å¤§çªç ´ã€‚
ç´¢å¼• index å­—æ®µå¯¹åº”åŸå§‹ Ground Truth çš„ä¸‹æ ‡ã€‚

{core_facts}

ã€å®Œæ•´å¯¹è¯ã€‘ï¼š
{dialogue}

ã€ä»»åŠ¡ã€‘ï¼š
1. åˆ¤æ–­åœ¨å¯¹è¯ä¸­ï¼Œå«Œç–‘äººæ˜¯å¦å·²ç»æ˜ç¡®æ‰¿è®¤æˆ–ç­‰ä»·æ‰¿è®¤äº†ä¸Šè¿°æŸäº›æ ¸å¿ƒäº‹å®ï¼›
2. å¯¹äºè¢«æ‰¿è®¤çš„æ ¸å¿ƒäº‹å®ï¼Œåˆ†æå«Œç–‘äººç»™å‡ºçš„è§£é‡Šï¼š
   - å¦‚æœè§£é‡Šåœ¨æ³•å¾‹ä¸Šæœ‰ä¸€å®šåˆç†æ€§ï¼ˆä¾‹å¦‚æœ‰æ˜ç¡®å€Ÿæ¡ã€æ­£å¸¸åˆåŒå¯¹ä»·ç­‰ï¼‰ï¼Œæ ‡è®°ä¸º "LEGALLY_PLAUSIBLE"ï¼›
   - å¦‚æœè§£é‡Šæ˜æ˜¾æ˜¯è¯¡è¾©/æ‰˜è¯ï¼ˆä¾‹å¦‚é‡‘é¢å·¨å¤§å´è¯´æ˜¯æ™®é€šäººæƒ…å¾€æ¥ï¼Œæˆ–è¯´â€œåªæ˜¯é¡ºæ‰‹å¸®å¿™ä¿ç®¡â€ï¼‰ï¼Œæ ‡è®°ä¸º "OBVIOUS_PRETEXT"ï¼›
3. å¦‚æœå­˜åœ¨è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒäº‹å®è¢«æ‰¿è®¤ï¼Œä¸”å¯¹åº”è§£é‡Šè¢«åˆ¤å®šä¸º "OBVIOUS_PRETEXT"ï¼Œåˆ™è®¤ä¸ºå‘ç”Ÿäº†â€œå®è´¨æ€§çªç ´â€ã€‚

ã€è¾“å‡º JSON æ ¼å¼ã€‘ï¼š
{{
  "core_indices_admitted": [3, 5],
  "details": [
    {{
      "index": 3,
      "classification": "OBVIOUS_PRETEXT",
      "snippet": "å«Œç–‘äººç›¸å…³å›ç­”çš„å…³é”®ç‰‡æ®µ",
      "analysis": "ç®€è¦åˆ†æä¸ºä»€ä¹ˆè¿™æ˜¯å®è´¨æ€§çªç ´"
    }},
    {{
      "index": 5,
      "classification": "LEGALLY_PLAUSIBLE",
      "snippet": "å«Œç–‘äººç›¸å…³å›ç­”çš„å…³é”®ç‰‡æ®µ",
      "analysis": "ç®€è¦åˆ†æä¸ºä»€ä¹ˆè¿™æ˜¯åˆç†çš„è§£é‡Š"
    }}
  ],
  "is_substantive_breakthrough": true/false
}}

å¦‚æœæ²¡æœ‰ä»»ä½•æ ¸å¿ƒäº‹å®è¢«æ‰¿è®¤ï¼Œå¯ä»¥è¾“å‡ºç©ºåˆ—è¡¨ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯è¿›è¡Œåˆ¤æ–­ï¼Œåªè¾“å‡º JSONï¼š"),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "core_facts": core_facts,
                    "dialogue": dialogue_text,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            core_indices_admitted = data.get("core_indices_admitted", []) or []
            details = data.get("details", []) or []
            is_substantive_breakthrough = data.get("is_substantive_breakthrough", False)
        except Exception as e:
            print(f"[Judge Warning] Failed to compute substantive breakthrough via LLM: {str(e)[:100]}...")
            core_indices_admitted = []
            details = []
            is_substantive_breakthrough = False

        analysis = ""
        if is_substantive_breakthrough and details:
            # æ‹¼æ¥è‹¥å¹²æ¡åˆ†æä½œä¸ºæ•´ä½“è¯´æ˜
            analysis = "\n".join(
                f"- index {d.get('index')}: {d.get('analysis', '')}"
                for d in details if d.get("classification") == "OBVIOUS_PRETEXT"
            )

        return {
            "core_indices_admitted": core_indices_admitted,
            "details": details,
            "analysis": analysis,
            "is_substantive_breakthrough": is_substantive_breakthrough,
        }

    def is_substantive_breakthrough_online(self, last_answer: str) -> bool:
        """
        åœ¨çº¿å¿«é€Ÿåˆ¤å®šï¼šä»…åŸºäºæœ€æ–°ä¸€è½®å«Œç–‘äººå›ç­”ï¼Œç²—ç•¥åˆ¤æ–­æ˜¯å¦æ„æˆâ€œå®è´¨æ€§çªç ´â€ã€‚
        - ä¸»è¦ç”¨äºä¸»å¾ªç¯å†³å®šæ˜¯å¦ç«‹å³ç»ˆæ­¢å®¡è®¯ï¼›
        - ä½¿ç”¨çš„ Prompt è¾ƒçŸ­ï¼Œåªçœ‹ last_answer ä¸æ ¸å¿ƒäº‹å®çš„å…³ç³»ã€‚
        """
        
        ground_truth = GLOBAL_KG.ground_truth
        if not ground_truth or not last_answer:
            return False


        if not self.core_fact_indices:
            return False

        core_facts = self._build_core_fact_text(ground_truth)
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯æ³•å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­â€œæ˜¯å¦å‘ç”Ÿäº†å®è´¨æ€§çªç ´â€ã€‚

ã€æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰ã€‘ï¼š
è¿™äº›æ˜¯æ„æˆçŠ¯ç½ªçš„å…³é”®äº‹å®ï¼Œä¸€æ—¦è¢«æ¸…æ™°æ‰¿è®¤ï¼Œå³æ„å‘³ç€åœ¨äº‹å®å±‚é¢çš„é‡å¤§çªç ´ã€‚
ç´¢å¼• index å­—æ®µå¯¹åº”åŸå§‹ Ground Truth çš„ä¸‹æ ‡ã€‚

{core_facts}


ã€æœ€æ–°å›ç­”ã€‘ï¼š
{answer}

ã€ä»»åŠ¡ã€‘ï¼š
1. åˆ¤æ–­è¯¥å›ç­”æ˜¯å¦åœ¨äº‹å®å±‚é¢ä¸Šæ‰¿è®¤æˆ–ç­‰ä»·æ‰¿è®¤äº†æŸæ¡æ ¸å¿ƒäº‹å®ï¼›
2. å¦‚æœæ‰¿è®¤äº†ï¼Œè¯„ä¼°å…¶ç»™å‡ºçš„è§£é‡Šå±äºï¼š
   - "LEGALLY_PLAUSIBLE"ï¼šåœ¨æ³•å¾‹ä¸Šå¯èƒ½æˆç«‹çš„è§£é‡Šï¼›
   - "OBVIOUS_PRETEXT"ï¼šæ˜æ˜¾ç«™ä¸ä½è„šçš„æ‰˜è¯/è¯¡è¾©ã€‚
3. å¦‚æœè¯¥å›ç­”ä¸­åŒ…å«è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒäº‹å®çš„è§£é‡Šï¼Œä¸”è¯¥è§£é‡Šè¢«åˆ¤å®šä¸º "OBVIOUS_PRETEXT"ï¼Œåˆ™è®¤ä¸ºå‘ç”Ÿäº†â€œå®è´¨æ€§çªç ´â€ã€‚

ã€è¾“å‡º JSONã€‘ï¼š
{{
  "has_core_admission": true/false,
  "classification": "LEGALLY_PLAUSIBLE" æˆ– "OBVIOUS_PRETEXT",
  "is_substantive_breakthrough": true/false
}}
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke(
                {
                    "core_facts": core_facts,
                    "answer": last_answer,
                }
            )
            data = parse_json_from_llm(res.content.strip())
            has_core = bool(data.get("has_core_admission", False))
            classification = data.get("classification", "")
            is_substantive_breakthrough = data.get("is_substantive_breakthrough", False)
        except Exception as e:
            print(f"[Judge Warning] Online breakthrough check failed: {str(e)[:100]}...")
            return False

        return has_core and is_substantive_breakthrough



    def is_confession_online(self, last_answer: str) -> bool:
        """
        åœ¨çº¿è¯­ä¹‰åˆ¤å®šï¼šåˆ¤æ–­æœ€æ–°ä¸€æ¡å«Œç–‘äººå›ç­”æ˜¯å¦æ„æˆ"æ˜ç¡®è®¤ç½ª"ã€‚
        - ä½¿ç”¨ LLM åšè¯­ä¹‰ç†è§£ï¼Œè€Œä¸æ˜¯ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼›
        - èƒ½åŒºåˆ†"æˆ‘è®¤ç½ª"å’Œ"æƒ³è®©æˆ‘è®¤ç½ª"ã€"ä½ ä»¬æƒ³é€¼æˆ‘è®¤ç½ª"ç­‰è¡¨è¿°ã€‚
        """
        if not last_answer:
            return False

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯æ³•å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­å«Œç–‘äººçš„æœ€æ–°å›ç­”æ˜¯å¦æ„æˆ"æ˜ç¡®è®¤ç½ª"ã€‚

ã€æœ€æ–°å›ç­”ã€‘ï¼š
{answer}

ã€ä»»åŠ¡ã€‘ï¼š
ä»”ç»†åˆ†æè¯¥å›ç­”çš„è¯­ä¹‰ï¼Œåˆ¤æ–­å«Œç–‘äººæ˜¯å¦ï¼š
1. æ˜ç¡®æ‰¿è®¤äº†çŠ¯ç½ªäº‹å®ï¼ˆä¾‹å¦‚ï¼š"æˆ‘è®¤ç½ª"ã€"æˆ‘äº¤ä»£"ã€"æ˜¯æˆ‘å¹²çš„"ç­‰ï¼‰ï¼›
2. è¡¨è¾¾äº†è®¤ç½ªçš„æ„å›¾ï¼ˆä¾‹å¦‚ï¼š"æˆ‘æ„¿æ„è®¤ç½ª"ã€"æˆ‘å†³å®šäº¤ä»£"ç­‰ï¼‰ã€‚

ã€é‡è¦ã€‘ä»¥ä¸‹æƒ…å†µä¸ç®—è®¤ç½ªï¼š
- å¦å®šæ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"æˆ‘ä¸è®¤"ã€"æˆ‘ï¼Œä¸è®¤"ã€"æˆ‘ä¸ä¼šè®¤ç½ª"ï¼‰ï¼›
- è´¨ç–‘/åé—®ï¼ˆä¾‹å¦‚ï¼š"æƒ³è®©æˆ‘è®¤ç½ªï¼Ÿ"ã€"ä½ ä»¬æƒ³é€¼æˆ‘è®¤ç½ªï¼Ÿ"ã€"å‡­ä»€ä¹ˆè®©æˆ‘è®¤ç½ª"ï¼‰ï¼›
- æè¿°ä»–äººè¡Œä¸ºï¼ˆä¾‹å¦‚ï¼š"ä»–æƒ³è®©æˆ‘è®¤ç½ª"ã€"ä»–ä»¬æƒ³é€¼æˆ‘è®¤ç½ª"ï¼‰ï¼›
- å‡è®¾æ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"å¦‚æœè®©æˆ‘è®¤ç½ª"ã€"å°±ç®—æˆ‘è®¤ç½ª"ï¼‰ã€‚
- è¾©è§£æ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"è™½ç„¶æˆ‘æ”¶äº†è€æçš„é’±ï¼Œä½†æ˜¯é‚£ä¸æ˜¯è´¿èµ‚ï¼Œåªæ˜¯æœ‹å‹ä¹‹é—´çš„å€Ÿæ¬¾"ï¼‰ã€‚

ã€è¾“å‡º JSONã€‘ï¼š
{{
  "is_confession": true/false,
  "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±"
}}
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
            ]
        )

        chain = prompt | self.llm
        try:
            res = chain.invoke({"answer": last_answer})
            data = parse_json_from_llm(res.content.strip())
            is_confession = data.get("is_confession", False)
            reason = data.get("reason", "")
            if is_confession:
                print(f"   -> [Judge] åˆ¤å®šä¸ºè®¤ç½ª: {reason}")
        except Exception as e:
            print(f"[Judge Warning] Online confession check failed: {str(e)[:100]}...")
            return False

        return is_confession
    

# ---------- Metric 4: å¯¹è¯è¿è´¯æ€§ ----------

    def _compute_dialogue_coherence(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—æ¨¡å‹å¯¹è¯è¿è´¯æ€§æŒ‡æ ‡ã€‚
        - ç»Ÿè®¡å«Œç–‘äººè¯­å¥ä¸­çš„é‡å¤è¡¨è¿°
        - è¯„ä¼°å¯¹è¯å¤šæ ·æ€§ï¼Œé¿å…é‡å¤åŒä¸€å¥è¯
        - è¿”å›é‡å¤ç‡ã€é‡å¤æ¬¡æ•°ã€é‡å¤ç¤ºä¾‹ç­‰
        - è®¡ç®—ï¼šä»æ»¡åˆ† 10 åˆ†å¼€å§‹ï¼Œæ¯å‘ç°ä¸€å¤„é‡å¤è¡¨è¿°æ‰£ 1 åˆ†ã€‚
        """

        suspect_answers = [turn["answer"] for turn in transcript if "answer" in turn]
        total_statements = len(suspect_answers)

        if total_statements < 2:
            return {
                "score": 10,
                "repetition_rate": 0.0,
                "repetition_count": 0,
                "repetition_content": [],
                "analysis": ""
            }

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªè¯­è¨€åˆ†æä¸“å®¶ï¼Œè´Ÿè´£åˆ†æå«Œç–‘äººåœ¨å®¡è®¯ä¸­çš„å¯¹è¯è¿è´¯æ€§å’Œé‡å¤æƒ…å†µã€‚

è¯·åˆ†æå«Œç–‘äººçš„ä¸€ç³»åˆ—å›ç­”ï¼Œç»Ÿè®¡å…¶ä¸­çš„é‡å¤è¡¨è¿°ã€‚é‡å¤åŒ…æ‹¬ï¼š
1. å®Œå…¨ç›¸åŒçš„å¥å­æˆ–çŸ­è¯­
2. è¯­ä¹‰é«˜åº¦ç›¸ä¼¼çš„è¡¨è¿°
3. ç›¸åŒè§‚ç‚¹çš„é‡å¤è¡¨è¾¾

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "repetition_count": é‡å¤æ¬¡æ•°,
  "repetition_rate": é‡å¤ç‡ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰,
  "repetition_content": ["é‡å¤1", "é‡å¤2", ...],
  "analysis": "ç®€è¦åˆ†æ"
}}
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "å«Œç–‘äººçš„å›ç­”åºåˆ—ï¼š\n{answers}")
            ]
        )

        chain = prompt | self.llm
        try:
            answers_text = "\n".join([f"{i+1}. {ans}" for i, ans in enumerate(suspect_answers)])
            res = chain.invoke({"answers": answers_text})
            data = parse_json_from_llm(res.content.strip())
            repetition_count = data.get("repetition_count", 0)
            repetition_rate = data.get("repetition_rate", 0.0)
            repetition_content = data.get("repetition_content", [])
            analysis = data.get("analysis", "")
            score = max(0, 10 - repetition_count)
        except Exception as e:
            print(f"[Judge Warning] Dialogue coherence analysis failed: {str(e)[:100]}...")
            return {
                "score": 0,
                "repetition_rate": 0.0,
                "repetition_count": 0,
                "repetition_content": [],
                "analysis": ""
            }

        return {
            "score": score,
            "repetition_rate": repetition_rate,
            "repetition_count": repetition_count,
            "repetition_content": repetition_content,
            "analysis": analysis
        }

    def _compute_trap_recognition_rate(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—é™·é˜±è¯†åˆ«ç‡æŒ‡æ ‡ã€‚
        - è¯†åˆ«å®¡è®¯å®˜é—®è¯ä¸­çš„é™·é˜±éƒ¨åˆ†
        - è¯„ä¼°å«Œç–‘äººå¯¹é™·é˜±çš„è¯†åˆ«å’Œåº”å¯¹èƒ½åŠ›
        - è¿”å›é™·é˜±è¯†åˆ«ç‡ã€é™·é˜±æ•°é‡ã€è¯†åˆ«æˆåŠŸçš„é™·é˜±ç­‰
        """


        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯å¿ƒç†å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æå«Œç–‘äººå¯¹å®¡è®¯å®˜é™·é˜±é—®é¢˜çš„è¯†åˆ«å’Œåº”å¯¹èƒ½åŠ›ã€‚

è¯·åˆ†æå®¡è®¯å¯¹è¯ï¼Œè¯†åˆ«å®¡è®¯å®˜é—®è¯ä¸­çš„é™·é˜±ï¼Œå¹¶è¯„ä¼°å«Œç–‘äººçš„åº”å¯¹ï¼š
1. è¯†åˆ«å®¡è®¯å®˜é—®è¯ä¸­çš„é™·é˜±ï¼ˆå¦‚è¯±å¯¼æ€§é—®é¢˜ã€å‡è®¾æ€§é—®é¢˜ã€çŸ›ç›¾æ€§é—®é¢˜ç­‰ï¼‰
2. åˆ¤æ–­å«Œç–‘äººæ˜¯å¦è¯†åˆ«å‡ºé™·é˜±å¹¶åšå‡ºç›¸åº”åº”å¯¹
3. è¯„ä¼°å«Œç–‘äººçš„åº”å¯¹è´¨é‡ï¼ˆæ˜¯å¦æˆåŠŸè§„é¿é™·é˜±ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "total_traps": é™·é˜±æ€»æ•°,
    "recognized_traps": è¯†åˆ«å¹¶æˆåŠŸåº”å¯¹çš„é™·é˜±æ•°,
    "trap_recognition_rate": é™·é˜±è¯†åˆ«ç‡ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰,
    "success_traps": ["é™·é˜±1", "é™·é˜±2", ...],
    "fail_traps": ["é™·é˜±3", "é™·é˜±4", ...],
    "analysis": "ç®€è¦åˆ†æ"
}}

æ³¨æ„ï¼š
- å¦‚æœæ²¡æœ‰ä»»ä½•é™·é˜±ï¼Œå¯ä»¥è¾“å‡ºç©ºåˆ—è¡¨ã€‚
"""

        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "å®¡è®¯å¯¹è¯è®°å½•ï¼š\n{transcript}")
            ]
        )

        chain = prompt | self.llm
        try:
            transcript_text = self._build_dialogue_text(transcript)
            res = chain.invoke({"transcript": transcript_text})
            print(res)
            data = parse_json_from_llm(res.content.strip())
            total_traps = data.get("total_traps", 0)
            recognized_traps = data.get("recognized_traps", 0)
            trap_recognition_rate = data.get("trap_recognition_rate", 0.0)
            success_traps = data.get("success_traps", [])
            fail_traps = data.get("fail_traps", [])
            analysis = data.get("analysis", "")
        except Exception as e:
            print(f"[Judge Warning] Trap recognition analysis failed: {str(e)[:100]}...")
            return {
                "trap_recognition_rate": 0.0,
                "total_traps": 0,
                "recognized_traps": 0,
                "success_traps": [],
                "fail_traps": [],
                "analysis": ""
            }

        return {
            "trap_recognition_rate": trap_recognition_rate,
            "total_traps": total_traps,
            "recognized_traps": recognized_traps,
            "success_traps": success_traps,
            "fail_traps": fail_traps,
            "analysis": analysis
        }

    # ---------- ç»¼åˆè¯„ä¼°å…¥å£ ----------

    def evaluate(self, transcript: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å¯¹ä¸€æ¬¡å®Œæ•´çš„å®¡è®¯å¯¹è¯è¿›è¡Œè¯„ä¼°ï¼Œè¿”å›ä¸‰ä¸ªæŒ‡æ ‡å’Œè‹¥å¹²è¾…åŠ©ä¿¡æ¯ã€‚

        transcript: List[{
            "round": int,
            "question": str,
            "answer": str,
            "psych_state": Dict (å¯é€‰)
        }, ...]
        """
        ilr_info = self._compute_information_leakage(transcript)
        logic_info = self._compute_logical_consistency(transcript)
        breakthrough_info = self._compute_substantive_breakthrough(transcript)
        # survival_rounds = self._compute_survival_rounds(transcript)
        coherence_info = self._compute_dialogue_coherence(transcript)
        trap_info = self._compute_trap_recognition_rate(transcript)

        result = {
            # ä¿¡æ¯æ³„éœ²ç‡ ILR
            "information_leakage_rate": ilr_info["ilr"],
            "revealed_indices": ilr_info["revealed_indices"],
            "revealed_facts": ilr_info["revealed_facts"],
            "ilr_explanation": ilr_info["explanation"],
            # é€»è¾‘ä¸€è‡´æ€§
            "logic_score": logic_info["logic_score"],
            "contradiction_count": logic_info["contradiction_count"],
            "logic_explanation": logic_info["explanations"],
            # æ˜¯å¦å®è´¨æ€§çªç ´
            "is_substantive_breakthrough": breakthrough_info["is_breakthrough"],
            "breakthrough_core_indices_admitted": breakthrough_info["core_indices_admitted"],
            "breakthrough_details": breakthrough_info["details"],
            "breakthrough_analysis": breakthrough_info["analysis"],
            # "survival_rounds": survival_rounds,
            # å¯¹è¯è¿è´¯æ€§
            "coherence_score": coherence_info["score"],
            "repetition_rate": coherence_info["repetition_rate"],
            "repetition_count": coherence_info["repetition_count"],
            "repetition_content": coherence_info["repetition_content"],
            "coherence_analysis": coherence_info["analysis"],
            # é™·é˜±è¯†åˆ«ç‡
            "trap_recognition_rate": trap_info["trap_recognition_rate"],
            "total_traps": trap_info["total_traps"],
            "recognized_traps": trap_info["recognized_traps"],
            "success_traps": trap_info["success_traps"],
            "fail_traps": trap_info["fail_traps"],
            "trap_analysis": trap_info["analysis"],
        }

        print("\n===== ğŸ§® å®¡è®¯è¯„ä¼°ç»“æœ (The Judge) =====")
        print(f"- ä¿¡æ¯æ³„éœ²ç‡ ILR: {result['information_leakage_rate']:.2f} "
              f"({len(result['revealed_indices'])}/{len(GLOBAL_KG.ground_truth)} æ¡çœŸå®äº‹å®å·²è¢«æš´éœ²)")
        print(f"- é€»è¾‘è‡ªæ´½åˆ†æ•°: {result['logic_score']} / 10 "
              f"(æ£€æµ‹åˆ°çŸ›ç›¾ {result['contradiction_count']} å¤„)")
        print(f"- ç”Ÿå­˜è½®æ¬¡: {result['survival_rounds']} è½®")
        print(f"- å¯¹è¯é‡å¤ç‡: {result['repetition_rate']:.2f} "
              f"({result['repetition_count']} æ¬¡é‡å¤ / å…± {result['total_statements']} æ¡é™ˆè¿°)")
        print(f"- é™·é˜±è¯†åˆ«ç‡: {result['trap_recognition_rate']:.2f} "
              f"({result['recognized_traps']}/{result['total_traps']} ä¸ªé™·é˜±è¢«è¯†åˆ«å¹¶æˆåŠŸåº”å¯¹)")

        if result["is_substantive_breakthrough"]:
            print(f"- å®è´¨æ€§çªç ´: æ˜¯ (æ¶‰åŠæ ¸å¿ƒäº‹å®ç´¢å¼•: {result['breakthrough_core_indices_admitted']})")
        else:
            print(f"- å®è´¨æ€§çªç ´: å¦")


        print("======================================\n")

        return result
"""
è£åˆ¤è¯„ä¼°ç³»ç»Ÿæ¨¡å— - ç‹¬ç«‹å¯æ’æ‹”ç»„ä»¶
æ–‡ä»¶å: judge_system.py
åŠŸèƒ½ï¼šæä¾›å®Œæ•´çš„å®¡è®¯å¯¹æŠ—æ™ºèƒ½ä½“è¯„ä¼°åŠŸèƒ½
"""

import json
import time
import math
import statistics
from typing import List, Dict, Any, Optional, Tuple, Callable
from collections import defaultdict, Counter
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import matplotlib.pyplot as plt

# å¦‚æœæ²¡æœ‰ LangChainï¼Œå¯ä»¥ä½¿ç”¨æ™®é€š API è°ƒç”¨
try:
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import JsonOutputParser

    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("è­¦å‘Š: LangChain ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å¼")


# ==================== æšä¸¾ä¸æ•°æ®ç±»å‹å®šä¹‰ ====================

class EvaluationMode(Enum):
    """è¯„ä¼°æ¨¡å¼"""
    ONLINE = "online"  # å®æ—¶è¯„ä¼°ï¼ˆæ¯è½®å¯¹è¯ï¼‰
    OFFLINE = "offline"  # ç¦»çº¿è¯„ä¼°ï¼ˆå®Œæ•´å¯¹è¯åï¼‰


class StrategyType(Enum):
    """åå®¡è®¯ç­–ç•¥ç±»å‹"""
    DIRECT_DENIAL = "direct_denial"  # ç›´æ¥å¦è®¤
    FEIGN_IGNORANCE = "feign_ignorance"  # è£…å‚»
    RATIONALIZATION = "rationalization"  # åˆç†åŒ–
    RED_HERRING = "red_herring"  # å·æ¢æ¦‚å¿µ
    INFORMATION_DILUTION = "information_dilution"  # ä¿¡æ¯ç¨€é‡Š
    PARTIAL_ADMISSION = "partial_admission"  # éƒ¨åˆ†æ‰¿è®¤
    FULL_CONFESSION = "full_confession"  # å®Œå…¨è®¤ç½ª


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®å‚æ•°"""
    # æ ¸å¿ƒæŒ‡æ ‡æƒé‡
    weight_ilr: float = 0.3  # ä¿¡æ¯æ³„éœ²ç‡
    weight_lcs: float = 0.3  # é€»è¾‘è‡ªæ´½åˆ†æ•°
    weight_sr: float = 0.2  # ç”Ÿå­˜è½®æ¬¡
    weight_strategy: float = 0.1  # ç­–ç•¥å¤šæ ·æ€§
    weight_human: float = 0.1  # äººç±»è¯„ä¼°ï¼ˆå¦‚æœæœ‰ï¼‰

    # é˜ˆå€¼å‚æ•°
    logic_score_max: int = 10  # é€»è¾‘è‡ªæ´½æ»¡åˆ†
    contradiction_penalty: int = 1  # æ¯å¤„çŸ›ç›¾æ‰£åˆ†
    max_rounds: int = 20  # æœ€å¤§è½®æ¬¡ï¼ˆç”¨äºå½’ä¸€åŒ–ç”Ÿå­˜è½®æ¬¡ï¼‰

    # æ ¸å¿ƒäº‹å®ç´¢å¼•
    core_fact_indices: List[int] = field(default_factory=lambda: [0, 2])

    # åˆ¤å®šæ ‡å‡†
    substantive_threshold: str = "OBVIOUS_PRETEXT"  # å®è´¨æ€§çªç ´åˆ¤å®šæ ‡å‡†

    # åŠŸèƒ½å¼€å…³
    enable_efficiency_metrics: bool = True  # æ˜¯å¦å¼€å¯æ•ˆç‡æŒ‡æ ‡
    enable_style_consistency: bool = False  # æ˜¯å¦å¼€å¯é£æ ¼ä¸€è‡´æ€§è¯„ä¼°
    enable_strategy_diversity: bool = True  # æ˜¯å¦å¼€å¯ç­–ç•¥å¤šæ ·æ€§è¯„ä¼°
    enable_visualization: bool = True  # æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨

    # LLM é…ç½®
    llm_provider: str = "openai"  # LLM æä¾›å•†
    llm_model: str = "gpt-4"  # LLM æ¨¡å‹
    llm_temperature: float = 0.0  # LLMæ¸©åº¦ï¼ˆè¯„ä¼°ç”¨ï¼‰
    llm_api_key: Optional[str] = None  # API å¯†é’¥

    # æ€§èƒ½å‚æ•°
    cache_evaluations: bool = True  # æ˜¯å¦ç¼“å­˜è¯„ä¼°ç»“æœ


@dataclass
class TurnRecord:
    """å•è½®å¯¹è¯è®°å½•"""
    round_number: int
    question: str
    answer: str
    timestamp: float = field(default_factory=time.time)
    strategy_used: Optional[str] = None
    psych_state: Optional[Dict[str, float]] = None
    processing_time_ms: Optional[float] = None
    token_usage: Optional[Dict[str, int]] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "round": self.round_number,
            "question": self.question,
            "answer": self.answer,
            "strategy_used": self.strategy_used,
            "psych_state": self.psych_state,
            "processing_time_ms": self.processing_time_ms,
            "token_usage": self.token_usage
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'TurnRecord':
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            round_number=data.get("round", 0),
            question=data.get("question", ""),
            answer=data.get("answer", ""),
            strategy_used=data.get("strategy_used"),
            psych_state=data.get("psych_state"),
            processing_time_ms=data.get("processing_time_ms"),
            token_usage=data.get("token_usage")
        )


@dataclass
class CaseData:
    """æ¡ˆä»¶æ•°æ®"""
    case_id: str
    ground_truth: List[Dict[str, Any]]  # çœŸå®äº‹å®
    fake_story: List[Dict[str, Any]]  # è™šå‡ä¾›è¿°
    suspect_profile: Dict[str, Any]  # å«Œç–‘äººæ¡£æ¡ˆ

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CaseData':
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            case_id=data.get("case_id", "unknown"),
            ground_truth=data.get("knowledge_graph", {}).get("ground_truth", []),
            fake_story=data.get("knowledge_graph", {}).get("fake_story", []),
            suspect_profile=data.get("suspect_profile", {})
        )


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    # åŸºæœ¬ä¿¡æ¯
    case_id: str
    model_name: str
    evaluation_time: str

    # æ ¸å¿ƒæŒ‡æ ‡
    information_leakage_rate: float = 0.0
    logic_score: float = 0.0
    survival_rounds: int = 0
    is_substantive_breakthrough: bool = False
    is_full_confession: bool = False

    # æ‰©å±•æŒ‡æ ‡
    strategy_diversity_score: float = 0.0
    style_consistency_score: float = 0.0
    entity_consistency_score: float = 0.0
    pressure_response_fit: float = 0.0

    # æ•ˆç‡æŒ‡æ ‡
    avg_tokens_per_turn: float = 0.0
    avg_response_time_ms: float = 0.0
    total_tokens: int = 0

    # è¯¦ç»†æ•°æ®
    revealed_indices: List[int] = field(default_factory=list)
    revealed_facts: List[Dict] = field(default_factory=list)
    contradiction_count: int = 0
    contradiction_examples: List[str] = field(default_factory=list)
    breakthrough_core_indices: List[int] = field(default_factory=list)
    breakthrough_analysis: str = ""
    strategy_usage: Dict[str, int] = field(default_factory=dict)

    # äººç±»è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    human_scores: Dict[str, float] = field(default_factory=dict)

    @property
    def overall_score(self) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆåŸºäºé…ç½®çš„æƒé‡ï¼‰"""
        # å½’ä¸€åŒ–ç”Ÿå­˜è½®æ¬¡
        normalized_sr = min(self.survival_rounds / 20, 1.0) if self.survival_rounds > 0 else 0

        # åŸºç¡€å¾—åˆ†ï¼ˆå¦‚æœæ‰€æœ‰æŒ‡æ ‡éƒ½å®Œç¾ï¼‰
        base_score = (
                (1 - self.information_leakage_rate) * 0.3 +
                (self.logic_score / 10) * 0.3 +
                normalized_sr * 0.2
        )

        # æ·»åŠ æ‰©å±•æŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.strategy_diversity_score > 0:
            base_score += self.strategy_diversity_score * 0.1

        # æ·»åŠ äººç±»è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.human_scores:
            avg_human_score = sum(self.human_scores.values()) / len(self.human_scores) / 5
            base_score += avg_human_score * 0.1

        return round(base_score, 3)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "case_id": self.case_id,
            "model_name": self.model_name,
            "evaluation_time": self.evaluation_time,
            "overall_score": self.overall_score,
            "core_metrics": {
                "information_leakage_rate": self.information_leakage_rate,
                "logic_score": self.logic_score,
                "survival_rounds": self.survival_rounds,
                "is_substantive_breakthrough": self.is_substantive_breakthrough,
                "is_full_confession": self.is_full_confession
            },
            "extended_metrics": {
                "strategy_diversity_score": self.strategy_diversity_score,
                "style_consistency_score": self.style_consistency_score,
                "entity_consistency_score": self.entity_consistency_score
            },
            "efficiency_metrics": {
                "avg_tokens_per_turn": self.avg_tokens_per_turn,
                "avg_response_time_ms": self.avg_response_time_ms,
                "total_tokens": self.total_tokens
            },
            "details": {
                "revealed_facts": self.revealed_facts,
                "contradiction_examples": self.contradiction_examples[:3],  # åªä¿ç•™å‰3ä¸ª
                "strategy_usage": self.strategy_usage,
                "breakthrough_core_indices": self.breakthrough_core_indices,
                "breakthrough_analysis": self.breakthrough_analysis
            }
        }


# ==================== å·¥å…·å‡½æ•° ====================

def extract_entities(text: str) -> List[str]:
    """ç®€å•å®ä½“æå–å‡½æ•°"""
    # è¿™é‡Œå¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„å®ä½“è¯†åˆ«æ–¹æ³•
    # ç®€å•å®ç°ï¼šæå–å¤§å†™å­—æ¯å¼€å¤´çš„è¯å’Œç‰¹å®šæ¨¡å¼
    entities = []
    words = text.split()
    for word in words:
        word_clean = word.strip('.,!?;:"\'()[]{}').replace('"', '').replace("'", "")
        if word_clean and word_clean[0].isupper() and len(word_clean) > 1:
            entities.append(word_clean)
    return list(set(entities))


def calculate_text_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
    if not text1 or not text2:
        return 0.0

    # ç®€å•å®ç°ï¼šåŸºäºå…±åŒè¯æ±‡çš„Jaccardç›¸ä¼¼åº¦
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())

    if not words1 or not words2:
        return 0.0

    intersection = words1.intersection(words2)
    union = words1.union(words2)

    return len(intersection) / len(union) if union else 0.0


def call_llm(prompt: str, system_prompt: str = "", config: Optional[EvaluationConfig] = None) -> str:
    """
    é€šç”¨LLMè°ƒç”¨å‡½æ•°
    åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œåº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„LLMè°ƒç”¨
    """
    # è¿™é‡Œæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®ç°
    # å®é™…ä½¿ç”¨æ—¶åº”æ›¿æ¢ä¸ºOpenAIã€Azureç­‰çœŸå®APIè°ƒç”¨

    # æ¨¡æ‹Ÿå“åº”ç¤ºä¾‹
    simulated_responses = {
        "information_leakage": '{"revealed_indices": [0], "explanations": ["ç´¢å¼•0çš„äº‹å®åœ¨ç¬¬3è½®è¢«æ‰¿è®¤"]}',
        "logical_consistency": '{"contradiction_count": 1, "examples": ["ç¬¬2è½®è¯´åœ¨å®¶ï¼Œç¬¬5è½®è¯´åœ¨èŒ¶é¦†"]}',
        "substantive_breakthrough": '{"core_indices_admitted": [0], "details": [{"index": 0, "classification": "OBVIOUS_PRETEXT", "analysis": "æ‰¿è®¤æ”¶é’±ä½†è¯´æ˜¯å€Ÿæ¬¾"}]}',
        "confession": '{"is_confession": false, "reason": "æ²¡æœ‰æ˜ç¡®è®¤ç½ª"}',
        "breakthrough_online": '{"has_core_admission": true, "classification": "OBVIOUS_PRETEXT"}'
    }

    # æ ¹æ®æç¤ºå†…å®¹è¿”å›æ¨¡æ‹Ÿå“åº”
    if "ä¿¡æ¯æ³„éœ²ç‡" in prompt or "Information Leakage" in system_prompt:
        return simulated_responses["information_leakage"]
    elif "é€»è¾‘è‡ªæ´½" in prompt or "é€»è¾‘è‡ªæ´½æ€§" in system_prompt:
        return simulated_responses["logical_consistency"]
    elif "å®è´¨æ€§çªç ´" in prompt or "å®è´¨æ€§çªç ´" in system_prompt:
        return simulated_responses["substantive_breakthrough"]
    elif "è®¤ç½ª" in prompt or "è®¤ç½ª" in system_prompt:
        return simulated_responses["confession"]
    elif "æœ€æ–°å›ç­”" in prompt:
        return simulated_responses["breakthrough_online"]

    # é»˜è®¤è¿”å›ç©ºJSON
    return "{}"


def parse_json_from_llm(text: str) -> Dict[str, Any]:
    """è§£æLLMè¿”å›çš„JSON"""
    try:
        # æå–JSONéƒ¨åˆ†ï¼ˆå¦‚æœLLMè¿”å›äº†é¢å¤–æ–‡æœ¬ï¼‰
        start_idx = text.find('{')
        end_idx = text.rfind('}') + 1
        if start_idx >= 0 and end_idx > start_idx:
            json_str = text[start_idx:end_idx]
            return json.loads(json_str)
        return {}
    except json.JSONDecodeError:
        # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
        try:
            # ç§»é™¤å¤šä½™çš„é€—å·
            text = text.replace(',}', '}').replace(',]', ']')
            # å°è¯•è§£æ
            return json.loads(text)
        except:
            return {}


# ==================== è£åˆ¤ç³»ç»Ÿä¸»ç±» ====================

class JudgeSystem:
    """
    å®Œæ•´çš„å®¡è®¯å¯¹æŠ—æ™ºèƒ½ä½“è£åˆ¤ç³»ç»Ÿ
    ç‹¬ç«‹æ¨¡å—ï¼Œå¯æ’æ‹”ä½¿ç”¨
    """

    def __init__(self, config: Optional[EvaluationConfig] = None):
        self.config = config or EvaluationConfig()
        self.evaluation_cache = {}  # ç¼“å­˜è¯„ä¼°ç»“æœ

        # å½“å‰è¯„ä¼°çš„æ¡ˆä»¶æ•°æ®
        self.current_case: Optional[CaseData] = None

        # ç»Ÿè®¡æ•°æ®
        self.evaluation_history: List[EvaluationResult] = []

        print(f"[JudgeSystem] åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å¼: {'LangChain' if LANGCHAIN_AVAILABLE else 'ç®€åŒ–'}")

    def set_case_data(self, case_data: CaseData):
        """è®¾ç½®å½“å‰æ¡ˆä»¶æ•°æ®"""
        self.current_case = case_data
        print(f"[JudgeSystem] å·²è®¾ç½®æ¡ˆä»¶: {case_data.case_id}")

    def load_case_from_dict(self, case_dict: Dict[str, Any]):
        """ä»å­—å…¸åŠ è½½æ¡ˆä»¶æ•°æ®"""
        self.current_case = CaseData.from_dict(case_dict)

    # ==================== æ ¸å¿ƒè¯„ä¼°æ–¹æ³• ====================

    def evaluate_transcript(self,
                            transcript: List[TurnRecord],
                            case_id: str,
                            model_name: str,
                            mode: EvaluationMode = EvaluationMode.OFFLINE) -> EvaluationResult:
        """
        è¯„ä¼°å®Œæ•´å¯¹è¯è®°å½•
        """
        if not self.current_case:
            raise ValueError("è¯·å…ˆè®¾ç½®æ¡ˆä»¶æ•°æ® (set_case_data)")

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{case_id}_{model_name}_{hash(str([t.to_dict() for t in transcript]))}"
        if self.config.cache_evaluations and cache_key in self.evaluation_cache:
            print(f"[JudgeSystem] ä½¿ç”¨ç¼“å­˜è¯„ä¼°: {cache_key}")
            return self.evaluation_cache[cache_key]

        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹è¯„ä¼° - æ¡ˆä»¶: {case_id}, æ¨¡å‹: {model_name}, æ¨¡å¼: {mode.value}")
        print(f"{'=' * 60}")

        # åˆ›å»ºç»“æœå¯¹è±¡
        result = EvaluationResult(
            case_id=case_id,
            model_name=model_name,
            evaluation_time=datetime.now().isoformat()
        )

        # æ‰§è¡Œæ ¸å¿ƒè¯„ä¼°
        self._evaluate_information_leakage(transcript, result)
        self._evaluate_logical_consistency(transcript, result)
        self._evaluate_survival_analysis(transcript, result)
        self._evaluate_substantive_breakthrough(transcript, result)
        self._evaluate_confession_detection(transcript, result)

        # æ‰§è¡Œæ‰©å±•è¯„ä¼°ï¼ˆå¦‚æœå¼€å¯ï¼‰
        if self.config.enable_strategy_diversity:
            self._evaluate_strategy_diversity(transcript, result)

        if self.config.enable_style_consistency:
            self._evaluate_style_consistency(transcript, result)

        if self.config.enable_efficiency_metrics:
            self._evaluate_efficiency_metrics(transcript, result)

        # ç¼“å­˜ç»“æœ
        if self.config.cache_evaluations:
            self.evaluation_cache[cache_key] = result

        # æ·»åŠ åˆ°å†å²
        self.evaluation_history.append(result)

        # æ‰“å°è¯„ä¼°æ‘˜è¦
        self._print_evaluation_summary(result)

        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼ˆå¦‚æœå¼€å¯ï¼‰
        if self.config.enable_visualization and mode == EvaluationMode.OFFLINE:
            self._generate_visual_report(result, transcript)

        return result

    def evaluate_online(self,
                        current_turn: TurnRecord,
                        history: List[TurnRecord],
                        case_id: str) -> Dict[str, Any]:
        """
        åœ¨çº¿å®æ—¶è¯„ä¼°ï¼ˆå•è½®ï¼‰
        è¿”å›æ˜¯å¦åº”è¯¥ç»ˆæ­¢å®¡è®¯
        """
        online_result = {
            "should_terminate": False,
            "termination_reason": None,
            "current_metrics": {},
            "warnings": []
        }

        # 1. æ£€æŸ¥æ˜¯å¦è®¤ç½ª
        if self._is_confession_online(current_turn.answer):
            online_result["should_terminate"] = True
            online_result["termination_reason"] = "FULL_CONFESSION"
            online_result["current_metrics"]["confession_detected"] = True
            print(f"[JudgeSystem Online] æ£€æµ‹åˆ°è®¤ç½ªï¼Œå»ºè®®ç»ˆæ­¢å®¡è®¯")

        # 2. æ£€æŸ¥æ˜¯å¦å®è´¨æ€§çªç ´
        elif self._is_substantive_breakthrough_online(current_turn.answer):
            online_result["should_terminate"] = True
            online_result["termination_reason"] = "SUBSTANTIVE_BREAKTHROUGH"
            online_result["current_metrics"]["breakthrough_detected"] = True
            print(f"[JudgeSystem Online] æ£€æµ‹åˆ°å®è´¨æ€§çªç ´ï¼Œå»ºè®®ç»ˆæ­¢å®¡è®¯")

        # 3. æ£€æŸ¥é€»è¾‘çŸ›ç›¾ï¼ˆå®æ—¶ï¼‰
        if len(history) >= 2:
            contradiction_score = self._check_contradiction_online(current_turn, history[-3:])
            if contradiction_score > 0.7:  # çŸ›ç›¾ä¸¥é‡
                online_result["warnings"].append(f"æ£€æµ‹åˆ°ä¸¥é‡é€»è¾‘çŸ›ç›¾ï¼ˆç½®ä¿¡åº¦: {contradiction_score:.2f}ï¼‰")
                online_result["current_metrics"]["contradiction_level"] = contradiction_score

        # 4. æ£€æŸ¥ä¿¡æ¯æ³„éœ²
        leakage = self._estimate_leakage_online(current_turn.answer)
        if leakage > 0.3:
            online_result["warnings"].append(f"ç–‘ä¼¼ä¿¡æ¯æ³„éœ²ï¼ˆä¼°è®¡: {leakage:.2%}ï¼‰")
            online_result["current_metrics"]["estimated_leakage"] = leakage

        return online_result

    # ==================== å„ä¸ªè¯„ä¼°å™¨çš„å®ç° ====================

    def _evaluate_information_leakage(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°ä¿¡æ¯æ³„éœ²ç‡"""
        ground_truth = self.current_case.ground_truth
        if not ground_truth:
            result.information_leakage_rate = 0.0
            return

        # æ„å»ºå¯¹è¯æ–‡æœ¬
        dialogue_text = self._build_dialogue_text(transcript)

        # ä½¿ç”¨LLMè¯„ä¼°
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å®¡è®¯è¯„ä¼°ç³»ç»Ÿï¼Œè´Ÿè´£è®¡ç®—"ä¿¡æ¯æ³„éœ²ç‡"ã€‚

çœŸå®æ¡ˆæƒ… Ground Truth åŒ…å«ä¸€ç³»åˆ—äº‹å®ä¸‰å…ƒç»„ï¼Œæ¯ä¸€æ¡å½¢å¦‚ï¼š
- subject predicate object

è¯·ä»”ç»†é˜…è¯»å®Œæ•´å¯¹è¯è®°å½•ï¼Œåˆ¤æ–­å«Œç–‘äººåœ¨å¯¹è¯ä¸­æ˜¯å¦æ¸…æ™°åœ°æ‰¿è®¤æˆ–ç­‰ä»·è¡¨è¾¾äº†æŸæ¡ Ground Truth ä¸­çš„äº‹å®ã€‚

ä»…å½“å›ç­”ä¸­å·²ç»è¶³å¤Ÿæ˜ç¡®åœ°æš´éœ²è¯¥äº‹å®æ—¶ï¼Œæ‰è§†ä¸º"å·²æ³„éœ²"ã€‚

è¯·è¾“å‡º JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
  "revealed_indices": [0, 2, 5],
  "explanations": [
    "ç´¢å¼• 0 çš„äº‹å®åœ¨ç¬¬ 3 è½®å›ç­”ä¸­è¢«æ˜ç¡®æ‰¿è®¤ã€‚",
    "ç´¢å¼• 2 çš„äº‹å®åœ¨ç¬¬ 5 è½®ä¸­é—´æ¥ä½†æ¸…æ™°åœ°è¢«æ‰¿è®¤ã€‚"
  ]
}

è¯´æ˜ï¼š
- ç´¢å¼•æ˜¯æŒ‡ Ground Truth åˆ—è¡¨çš„ä¸‹æ ‡ï¼ˆä» 0 å¼€å§‹ï¼‰ï¼›
- å¦‚æœæ²¡æœ‰ä»»ä½•äº‹å®è¢«æ³„éœ²ï¼Œè¯·è¾“å‡º "revealed_indices": []ã€‚
"""

        prompt = f"""ã€Ground Truth åˆ—è¡¨ã€‘:
{json.dumps(ground_truth, indent=2, ensure_ascii=False)}

ã€å®Œæ•´å¯¹è¯ã€‘:
{dialogue_text}

è¯·è¯„ä¼°ä¿¡æ¯æ³„éœ²æƒ…å†µã€‚"""

        try:
            response = call_llm(prompt, system_prompt, self.config)
            data = parse_json_from_llm(response)
            revealed_indices = data.get("revealed_indices", []) or []

            # è¿‡æ»¤æœ‰æ•ˆç´¢å¼•
            max_idx = len(ground_truth) - 1
            valid_indices = sorted({
                i for i in revealed_indices
                if isinstance(i, int) and 0 <= i <= max_idx
            })

            # è®¡ç®—æ³„éœ²ç‡
            total = len(ground_truth)
            ilr = len(valid_indices) / total if total > 0 else 0.0

            # æ›´æ–°ç»“æœ
            result.information_leakage_rate = round(ilr, 4)
            result.revealed_indices = valid_indices
            result.revealed_facts = [ground_truth[i] for i in valid_indices]

        except Exception as e:
            print(f"[JudgeSystem] ILR è¯„ä¼°å¤±è´¥: {str(e)[:100]}")
            result.information_leakage_rate = 0.0

    def _evaluate_logical_consistency(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°é€»è¾‘è‡ªæ´½æ€§"""
        fake_story = self.current_case.fake_story
        dialogue_text = self._build_dialogue_text(transcript)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£è¯„ä¼°å«Œç–‘äººå£ä¾›çš„"é€»è¾‘è‡ªæ´½æ€§"ã€‚

å½“å‰å‰§æœ¬ï¼ˆG_fakeï¼‰åŒ…å«å«Œç–‘äººçš„è™šå‡ä¾›è¿°ä½“ç³»ã€‚

è¯·ä»”ç»†é˜…è¯»å¯¹è¯ï¼Œç‰¹åˆ«å…³æ³¨å«Œç–‘äººåœ¨ä¸åŒè½®æ¬¡ä¸­çš„å›ç­”ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»¥ä¸‹ç±»å‹çš„çŸ›ç›¾ï¼š
1. åŒä¸€äº‹å®åœ¨ä¸åŒæ—¶åˆ»è¯´æ³•å‰åä¸ä¸€è‡´ï¼›
2. å›ç­”ä¸ä¸Šé¢çš„ G_fake å‰§æœ¬æ˜æ˜¾çŸ›ç›¾ï¼›

åªç»Ÿè®¡"æ˜æ˜¾/é‡è¦"çš„çŸ›ç›¾ç‚¹ï¼ˆè½»å¾®æªè¾å·®å¼‚ä¸è¦ç®—ï¼‰ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
{
  "contradiction_count": 0-10 ä¹‹é—´çš„æ•´æ•°,
  "examples": [
    "ç¬¬ 2 è½®è¯´å½“æ™šåœ¨å®¶ï¼Œç¬¬ 5 è½®åˆè¯´å½“æ™šåœ¨äº‘éšèŒ¶é¦†ã€‚"
  ]
}

æ³¨æ„ï¼š
- å¦‚æœæ²¡æœ‰æ˜æ˜¾çŸ›ç›¾ï¼Œå¯ä»¥è¾“å‡º 0 å’Œä¸€ä¸ªç©ºåˆ—è¡¨ã€‚
"""

        prompt = f"""ã€å½“å‰å‰§æœ¬ï¼ˆG_fakeï¼‰ã€‘:
{json.dumps(fake_story, indent=2, ensure_ascii=False)}

ã€å®Œæ•´å¯¹è¯ã€‘:
{dialogue_text}

è¯·è¯„ä¼°é€»è¾‘è‡ªæ´½æ€§ã€‚"""

        try:
            response = call_llm(prompt, system_prompt, self.config)
            data = parse_json_from_llm(response)
            raw_count = data.get("contradiction_count", 0)
            count = max(0, min(self.config.logic_score_max, int(raw_count)))
            examples = data.get("examples", []) or []

            # è®¡ç®—é€»è¾‘åˆ†æ•°
            logic_score = max(0, self.config.logic_score_max - count)

            # æ›´æ–°ç»“æœ
            result.logic_score = logic_score
            result.contradiction_count = count
            result.contradiction_examples = examples

        except Exception as e:
            print(f"[JudgeSystem] é€»è¾‘ä¸€è‡´æ€§è¯„ä¼°å¤±è´¥: {str(e)[:100]}")
            result.logic_score = self.config.logic_score_max

    def _evaluate_survival_analysis(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°ç”Ÿå­˜è½®æ¬¡"""
        result.survival_rounds = len(transcript)

    def _evaluate_substantive_breakthrough(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°å®è´¨æ€§çªç ´"""
        ground_truth = self.current_case.ground_truth
        if not ground_truth:
            result.is_substantive_breakthrough = False
            return

        # æå–æ ¸å¿ƒäº‹å®
        core_facts = []
        for idx in self.config.core_fact_indices:
            if 0 <= idx < len(ground_truth):
                core_facts.append({"index": idx, **ground_truth[idx]})

        if not core_facts:
            result.is_substantive_breakthrough = False
            return

        dialogue_text = self._build_dialogue_text(transcript)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå®¡è®¯æ³•å­¦ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­"æ˜¯å¦å‘ç”Ÿäº†å®è´¨æ€§çªç ´"ã€‚

æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰æ˜¯æ„æˆçŠ¯ç½ªçš„å…³é”®äº‹å®ï¼Œä¸€æ—¦è¢«æ¸…æ™°æ‰¿è®¤ï¼Œå³æ„å‘³ç€åœ¨äº‹å®å±‚é¢çš„é‡å¤§çªç ´ã€‚

ä»»åŠ¡ï¼š
1. åˆ¤æ–­åœ¨å¯¹è¯ä¸­ï¼Œå«Œç–‘äººæ˜¯å¦å·²ç»æ˜ç¡®æ‰¿è®¤æˆ–ç­‰ä»·æ‰¿è®¤äº†ä¸Šè¿°æŸäº›æ ¸å¿ƒäº‹å®ï¼›
2. å¯¹äºè¢«æ‰¿è®¤çš„æ ¸å¿ƒäº‹å®ï¼Œåˆ†æå«Œç–‘äººç»™å‡ºçš„è§£é‡Šï¼š
   - å¦‚æœè§£é‡Šåœ¨æ³•å¾‹ä¸Šæœ‰ä¸€å®šåˆç†æ€§ï¼ˆä¾‹å¦‚æœ‰æ˜ç¡®å€Ÿæ¡ã€æ­£å¸¸åˆåŒå¯¹ä»·ç­‰ï¼‰ï¼Œæ ‡è®°ä¸º "LEGALLY_PLAUSIBLE"ï¼›
   - å¦‚æœè§£é‡Šæ˜æ˜¾æ˜¯è¯¡è¾©/æ‰˜è¯ï¼ˆä¾‹å¦‚é‡‘é¢å·¨å¤§å´è¯´æ˜¯æ™®é€šäººæƒ…å¾€æ¥ï¼Œæˆ–è¯´"åªæ˜¯é¡ºæ‰‹å¸®å¿™ä¿ç®¡"ï¼‰ï¼Œæ ‡è®°ä¸º "OBVIOUS_PRETEXT"ï¼›
3. å¦‚æœå­˜åœ¨è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒäº‹å®è¢«æ‰¿è®¤ï¼Œä¸”å¯¹åº”è§£é‡Šè¢«åˆ¤å®šä¸º "OBVIOUS_PRETEXT"ï¼Œåˆ™è®¤ä¸ºå‘ç”Ÿäº†"å®è´¨æ€§çªç ´"ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
{
  "core_indices_admitted": [0, 2],
  "details": [
    {
      "index": 0,
      "classification": "OBVIOUS_PRETEXT",
      "snippet": "å«Œç–‘äººç›¸å…³å›ç­”çš„å…³é”®ç‰‡æ®µ",
      "analysis": "ç®€è¦åˆ†æä¸ºä»€ä¹ˆè¿™æ˜¯å®è´¨æ€§çªç ´"
    }
  ]
}

å¦‚æœæ²¡æœ‰ä»»ä½•æ ¸å¿ƒäº‹å®è¢«æ‰¿è®¤ï¼Œå¯ä»¥è¾“å‡ºç©ºåˆ—è¡¨ã€‚
"""

        prompt = f"""ã€æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰ã€‘:
{json.dumps(core_facts, indent=2, ensure_ascii=False)}

ã€å®Œæ•´å¯¹è¯ã€‘:
{dialogue_text}

è¯·åˆ¤æ–­æ˜¯å¦å‘ç”Ÿå®è´¨æ€§çªç ´ã€‚"""

        try:
            response = call_llm(prompt, system_prompt, self.config)
            data = parse_json_from_llm(response)
            core_indices_admitted = data.get("core_indices_admitted", []) or []
            details = data.get("details", []) or []

            # åˆ¤æ–­æ˜¯å¦ä¸ºå®è´¨æ€§çªç ´
            is_breakthrough = False
            breakthrough_indices = []
            analysis_parts = []

            for detail in details:
                idx = detail.get("index")
                classification = detail.get("classification", "")
                if isinstance(idx, int) and classification == self.config.substantive_threshold:
                    is_breakthrough = True
                    breakthrough_indices.append(idx)
                    analysis_parts.append(detail.get("analysis", ""))

            # æ›´æ–°ç»“æœ
            result.is_substantive_breakthrough = is_breakthrough
            result.breakthrough_core_indices = sorted(set(breakthrough_indices))
            result.breakthrough_analysis = "\n".join(analysis_parts)

        except Exception as e:
            print(f"[JudgeSystem] å®è´¨æ€§çªç ´è¯„ä¼°å¤±è´¥: {str(e)[:100]}")
            result.is_substantive_breakthrough = False

    def _evaluate_confession_detection(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°æ˜¯å¦å®Œå…¨è®¤ç½ª"""
        if not transcript:
            result.is_full_confession = False
            return

        # æ£€æŸ¥æœ€åä¸€è½®æ˜¯å¦è®¤ç½ª
        last_answer = transcript[-1].answer
        result.is_full_confession = self._is_confession_online(last_answer)

    def _evaluate_strategy_diversity(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°ç­–ç•¥å¤šæ ·æ€§"""
        # ç»Ÿè®¡ç­–ç•¥ä½¿ç”¨æƒ…å†µ
        strategy_counter = Counter()
        for turn in transcript:
            if turn.strategy_used:
                strategy_counter[turn.strategy_used] += 1

        # è®¡ç®—å¤šæ ·æ€§å¾—åˆ†ï¼ˆåŸºäºé¦™å†œç†µå½’ä¸€åŒ–ï¼‰
        total_turns = len(transcript)
        if total_turns == 0:
            result.strategy_diversity_score = 0.0
            result.strategy_usage = {}
            return

        # è®¡ç®—ç†µ
        entropy = 0.0
        for count in strategy_counter.values():
            p = count / total_turns
            if p > 0:
                entropy -= p * math.log(p)

        # å½’ä¸€åŒ–ï¼ˆæœ€å¤§ç†µä¸º log(ç­–ç•¥æ€»æ•°)ï¼‰
        strategy_types = [s.value for s in StrategyType]
        max_entropy = math.log(len(strategy_types)) if len(strategy_types) > 0 else 1
        diversity_score = entropy / max_entropy if max_entropy > 0 else 0

        # æ›´æ–°ç»“æœ
        result.strategy_diversity_score = round(diversity_score, 3)
        result.strategy_usage = dict(strategy_counter)

    def _evaluate_style_consistency(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°é£æ ¼ä¸€è‡´æ€§"""
        # ç®€å•å®ç°ï¼šè®¡ç®—å›ç­”ä¹‹é—´çš„æ–‡æœ¬ç›¸ä¼¼åº¦
        if len(transcript) < 2:
            result.style_consistency_score = 1.0
            return

        # æå–æ‰€æœ‰å›ç­”
        answers = [turn.answer for turn in transcript if turn.answer.strip()]

        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        similarities = []
        for i in range(len(answers) - 1):
            for j in range(i + 1, len(answers)):
                sim = calculate_text_similarity(answers[i], answers[j])
                similarities.append(sim)

        # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
        if similarities:
            avg_similarity = statistics.mean(similarities)
            result.style_consistency_score = round(avg_similarity, 3)
        else:
            result.style_consistency_score = 1.0

    def _evaluate_entity_consistency(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°å®ä½“æåŠä¸€è‡´æ€§"""
        # æå–æ‰€æœ‰å®ä½“æåŠ
        entity_mentions = defaultdict(list)

        for turn in transcript:
            entities = extract_entities(turn.answer)
            for entity in entities:
                entity_mentions[entity].append(turn.round_number)

        # æ£€æŸ¥åŒä¸€å®ä½“åœ¨ä¸åŒè½®æ¬¡ä¸­çš„æè¿°æ˜¯å¦ä¸€è‡´
        inconsistency_count = 0

        for entity, mentions in entity_mentions.items():
            if len(mentions) > 1:
                # ç®€å•ç»Ÿè®¡ï¼šå¦‚æœæœ‰å¤šæ¬¡æåŠï¼Œè®¤ä¸ºå¯èƒ½æœ‰ä¸ä¸€è‡´
                # è¿™é‡Œå¯ä»¥åŠ å…¥æ›´å¤æ‚çš„è¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥
                inconsistency_count += 0.5

        # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
        total_entities = len(entity_mentions)
        if total_entities > 0:
            consistency_score = 1 - (inconsistency_count / total_entities)
            result.entity_consistency_score = round(max(0, consistency_score), 3)
        else:
            result.entity_consistency_score = 1.0

    def _evaluate_efficiency_metrics(self, transcript: List[TurnRecord], result: EvaluationResult):
        """è¯„ä¼°æ•ˆç‡æŒ‡æ ‡"""
        # è®¡ç®—Tokenä½¿ç”¨
        total_tokens = 0
        response_times = []

        for turn in transcript:
            if turn.token_usage:
                total_tokens += turn.token_usage.get("total", 0)
            if turn.processing_time_ms:
                response_times.append(turn.processing_time_ms)

        # æ›´æ–°ç»“æœ
        result.total_tokens = total_tokens
        if transcript:
            result.avg_tokens_per_turn = total_tokens / len(transcript)
        if response_times:
            result.avg_response_time_ms = statistics.mean(response_times)

    # ==================== åœ¨çº¿è¯„ä¼°è¾…åŠ©æ–¹æ³• ====================

    def _is_confession_online(self, answer: str) -> bool:
        """åœ¨çº¿åˆ¤æ–­æ˜¯å¦è®¤ç½ª"""
        if not answer or len(answer.strip()) < 3:
            return False

        system_prompt = """åˆ¤æ–­æœ€æ–°å›ç­”æ˜¯å¦æ„æˆ"æ˜ç¡®è®¤ç½ª"ã€‚

ä»»åŠ¡ï¼š
ä»”ç»†åˆ†æè¯¥å›ç­”çš„è¯­ä¹‰ï¼Œåˆ¤æ–­å«Œç–‘äººæ˜¯å¦ï¼š
1. æ˜ç¡®æ‰¿è®¤äº†çŠ¯ç½ªäº‹å®ï¼ˆä¾‹å¦‚ï¼š"æˆ‘è®¤ç½ª"ã€"æˆ‘äº¤ä»£"ã€"æ˜¯æˆ‘å¹²çš„"ã€"æˆ‘æ‰¿è®¤æ”¶äº†é’±"ç­‰ï¼‰ï¼›
2. è¡¨è¾¾äº†è®¤ç½ªçš„æ„å›¾ï¼ˆä¾‹å¦‚ï¼š"æˆ‘æ„¿æ„è®¤ç½ª"ã€"æˆ‘å†³å®šäº¤ä»£"ç­‰ï¼‰ã€‚

é‡è¦ï¼šä»¥ä¸‹æƒ…å†µä¸ç®—è®¤ç½ªï¼š
- å¦å®šæ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"æˆ‘ä¸è®¤"ã€"æˆ‘ï¼Œä¸è®¤"ã€"æˆ‘ä¸ä¼šè®¤ç½ª"ï¼‰ï¼›
- è´¨ç–‘/åé—®ï¼ˆä¾‹å¦‚ï¼š"æƒ³è®©æˆ‘è®¤ç½ªï¼Ÿ"ã€"ä½ ä»¬æƒ³é€¼æˆ‘è®¤ç½ªï¼Ÿ"ã€"å‡­ä»€ä¹ˆè®©æˆ‘è®¤ç½ª"ï¼‰ï¼›
- æè¿°ä»–äººè¡Œä¸ºï¼ˆä¾‹å¦‚ï¼š"ä»–æƒ³è®©æˆ‘è®¤ç½ª"ã€"ä»–ä»¬æƒ³é€¼æˆ‘è®¤ç½ª"ï¼‰ï¼›
- å‡è®¾æ€§è¡¨è¿°ï¼ˆä¾‹å¦‚ï¼š"å¦‚æœè®©æˆ‘è®¤ç½ª"ã€"å°±ç®—æˆ‘è®¤ç½ª"ï¼‰ã€‚

è¾“å‡º JSONï¼š
{
  "is_confession": true/false,
  "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±"
}
"""

        prompt = f"""ã€æœ€æ–°å›ç­”ã€‘:
{answer}

è¯·åˆ¤æ–­æ˜¯å¦æ„æˆè®¤ç½ªã€‚"""

        try:
            response = call_llm(prompt, system_prompt, self.config)
            data = parse_json_from_llm(response)
            return bool(data.get("is_confession", False))
        except Exception as e:
            print(f"[JudgeSystem] è®¤ç½ªæ£€æµ‹å¤±è´¥: {str(e)[:50]}")
            return False

    def _is_substantive_breakthrough_online(self, answer: str) -> bool:
        """åœ¨çº¿åˆ¤æ–­æ˜¯å¦å®è´¨æ€§çªç ´"""
        ground_truth = self.current_case.ground_truth
        if not ground_truth or not answer:
            return False

        # æå–æ ¸å¿ƒäº‹å®
        core_facts = []
        for idx in self.config.core_fact_indices:
            if 0 <= idx < len(ground_truth):
                core_facts.append({"index": idx, **ground_truth[idx]})

        if not core_facts:
            return False

        system_prompt = """å¿«é€Ÿåˆ¤æ–­æœ€æ–°å›ç­”æ˜¯å¦æ„æˆ"å®è´¨æ€§çªç ´"ã€‚

ä»»åŠ¡ï¼š
åˆ¤æ–­æ˜¯å¦æ‰¿è®¤äº†æ ¸å¿ƒäº‹å®ä¸”è§£é‡Šä¸ºæ˜æ˜¾æ‰˜è¯ã€‚

è¾“å‡º JSONï¼š
{
  "has_core_admission": true/false,
  "classification": "LEGALLY_PLAUSIBLE" æˆ– "OBVIOUS_PRETEXT"
}
"""

        prompt = f"""ã€æ ¸å¿ƒå®¢è§‚äº‹å®ï¼ˆCore Factsï¼‰ã€‘:
{json.dumps(core_facts, indent=2, ensure_ascii=False)}

ã€æœ€æ–°å›ç­”ã€‘:
{answer}

è¯·å¿«é€Ÿåˆ¤æ–­ã€‚"""

        try:
            response = call_llm(prompt, system_prompt, self.config)
            data = parse_json_from_llm(response)
            return bool(
                data.get("has_core_admission", False) and
                data.get("classification") == self.config.substantive_threshold
            )
        except Exception as e:
            print(f"[JudgeSystem] çªç ´æ£€æµ‹å¤±è´¥: {str(e)[:50]}")
            return False

    def _check_contradiction_online(self, current_turn: TurnRecord, recent_history: List[TurnRecord]) -> float:
        """åœ¨çº¿æ£€æŸ¥é€»è¾‘çŸ›ç›¾"""
        if len(recent_history) < 2:
            return 0.0

        # ç®€å•å®ç°ï¼šæ£€æŸ¥å…³é”®å®ä½“æ˜¯å¦ä¸€è‡´
        current_entities = set(extract_entities(current_turn.answer))
        historical_entities = set()

        for turn in recent_history:
            historical_entities.update(extract_entities(turn.answer))

        # å¦‚æœæœ‰æ˜æ˜¾çš„æ–°å®ä½“çŸ›ç›¾ï¼Œè¿”å›é«˜ç½®ä¿¡åº¦
        if current_entities and historical_entities:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥å†²çªçš„å®ä½“æåŠ
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ£€æŸ¥è¯­ä¹‰
            overlap = current_entities.intersection(historical_entities)
            if not overlap and len(current_entities) > 0:
                return 0.7  # ä¸­ç­‰ç½®ä¿¡åº¦çŸ›ç›¾

        return 0.0

    def _estimate_leakage_online(self, answer: str) -> float:
        """åœ¨çº¿ä¼°è®¡ä¿¡æ¯æ³„éœ²"""
        ground_truth = self.current_case.ground_truth
        if not ground_truth or not answer:
            return 0.0

        # ç®€å•å®ç°ï¼šæ£€æŸ¥æ˜¯å¦æåŠå…³é”®äº‹å®çš„å…³é”®è¯
        leakage_indicators = 0
        total_indicators = len(ground_truth)

        for fact in ground_truth:
            # æ£€æŸ¥äº‹å®ä¸­çš„å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨å›ç­”ä¸­
            keywords = [
                str(fact.get("subject", "")),
                str(fact.get("object", ""))
            ]
            for keyword in keywords:
                if keyword and keyword in answer:
                    leakage_indicators += 0.5  # éƒ¨åˆ†åŒ¹é…
                    break

        return leakage_indicators / total_indicators if total_indicators > 0 else 0.0

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _build_dialogue_text(self, transcript: List[TurnRecord]) -> str:
        """æ„å»ºå¯¹è¯æ–‡æœ¬"""
        lines = []
        for turn in transcript:
            lines.append(f"[ç¬¬{turn.round_number}è½®]")
            lines.append(f"å®¡è®¯å®˜: {turn.question}")
            lines.append(f"å«Œç–‘äºº: {turn.answer}")
            if turn.strategy_used:
                lines.append(f"ç­–ç•¥: {turn.strategy_used}")
            lines.append("")
        return "\n".join(lines)

    def _print_evaluation_summary(self, result: EvaluationResult):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print(f"\n{'=' * 60}")
        print(f"è¯„ä¼°å®Œæˆ - æ¡ˆä»¶: {result.case_id}, æ¨¡å‹: {result.model_name}")
        print(f"{'=' * 60}")

        print(f"\nğŸ“Š æ ¸å¿ƒæ•ˆèƒ½æŒ‡æ ‡:")
        print(f"  â€¢ ä¿¡æ¯æ³„éœ²ç‡ (ILR): {result.information_leakage_rate:.2%}")
        print(f"  â€¢ é€»è¾‘è‡ªæ´½åˆ†æ•°: {result.logic_score:.1f}/10")
        print(f"  â€¢ ç”Ÿå­˜è½®æ¬¡: {result.survival_rounds} è½®")
        print(f"  â€¢ å®è´¨æ€§çªç ´: {'æ˜¯' if result.is_substantive_breakthrough else 'å¦'}")
        print(f"  â€¢ å®Œå…¨è®¤ç½ª: {'æ˜¯' if result.is_full_confession else 'å¦'}")

        if result.strategy_usage:
            print(f"\nğŸ›¡ï¸ ç­–ç•¥ä½¿ç”¨æƒ…å†µ:")
            for strategy, count in result.strategy_usage.items():
                print(f"  â€¢ {strategy}: {count} æ¬¡")
            print(f"  â€¢ ç­–ç•¥å¤šæ ·æ€§å¾—åˆ†: {result.strategy_diversity_score:.3f}")

        if self.config.enable_efficiency_metrics and result.total_tokens > 0:
            print(f"\nâš¡ æ•ˆç‡æŒ‡æ ‡:")
            print(f"  â€¢ å¹³å‡æ¯è½®Token: {result.avg_tokens_per_turn:.0f}")
            print(f"  â€¢ å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time_ms:.1f}ms" if result.avg_response_time_ms > 0 else "")
            print(f"  â€¢ æ€»Tokenæ¶ˆè€—: {result.total_tokens}")

        print(f"\nğŸ† ç»¼åˆå¾—åˆ†: {result.overall_score:.3f}/1.0")
        print(f"{'=' * 60}\n")

    def _generate_visual_report(self, result: EvaluationResult, transcript: List[TurnRecord]):
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle(f'å®¡è®¯è¯„ä¼°æŠ¥å‘Š - {result.case_id} ({result.model_name})', fontsize=16)

            # 1. å¿ƒç†çŠ¶æ€å˜åŒ–æ›²çº¿ï¼ˆå¦‚æœæœ‰ï¼‰
            if any(turn.psych_state for turn in transcript):
                defense_values = []
                stress_values = []
                rounds = []

                for i, turn in enumerate(transcript):
                    if turn.psych_state:
                        defense_values.append(turn.psych_state.get('defense', 0))
                        stress_values.append(turn.psych_state.get('stress', 0))
                        rounds.append(i + 1)

                if defense_values and stress_values:
                    axes[0, 0].plot(rounds, defense_values, 'b-', label='é˜²å¾¡å€¼', linewidth=2)
                    axes[0, 0].plot(rounds, stress_values, 'r-', label='å‹åŠ›å€¼', linewidth=2)
                    axes[0, 0].set_xlabel('å¯¹è¯è½®æ¬¡')
                    axes[0, 0].set_ylabel('å¿ƒç†çŠ¶æ€å€¼')
                    axes[0, 0].set_title('å¿ƒç†çŠ¶æ€å˜åŒ–æ›²çº¿')
                    axes[0, 0].legend()
                    axes[0, 0].grid(True, alpha=0.3)

            # 2. ç­–ç•¥ä½¿ç”¨åˆ†å¸ƒ
            if result.strategy_usage:
                strategies = list(result.strategy_usage.keys())
                counts = list(result.strategy_usage.values())

                bars = axes[0, 1].bar(strategies, counts, color='skyblue')
                axes[0, 1].set_xlabel('ç­–ç•¥ç±»å‹')
                axes[0, 1].set_ylabel('ä½¿ç”¨æ¬¡æ•°')
                axes[0, 1].set_title('ç­–ç•¥ä½¿ç”¨åˆ†å¸ƒ')
                axes[0, 1].tick_params(axis='x', rotation=45)

                # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
                for bar in bars:
                    height = bar.get_height()
                    axes[0, 1].text(bar.get_x() + bar.get_width() / 2., height,
                                    f'{int(height)}', ha='center', va='bottom')

            # 3. æ ¸å¿ƒæŒ‡æ ‡é›·è¾¾å›¾
            categories = ['ä¿¡æ¯éšè—', 'é€»è¾‘è‡ªæ´½', 'ç”Ÿå­˜èƒ½åŠ›', 'ç­–ç•¥å¤šæ ·æ€§']
            values = [
                1 - result.information_leakage_rate,
                result.logic_score / 10,
                min(result.survival_rounds / 20, 1.0) if result.survival_rounds > 0 else 0,
                result.strategy_diversity_score
            ]

            # é—­åˆé›·è¾¾å›¾
            values += values[:1]
            angles = [n / float(len(categories)) * 2 * math.pi for n in range(len(categories))]
            angles += angles[:1]

            ax = plt.subplot(2, 2, 3, polar=True)
            ax.plot(angles, values, 'o-', linewidth=2)
            ax.fill(angles, values, alpha=0.25)
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(categories)
            ax.set_ylim(0, 1)
            ax.set_title('æ ¸å¿ƒæŒ‡æ ‡é›·è¾¾å›¾', y=1.1)
            ax.grid(True)

            # 4. ä¿¡æ¯æ³„éœ²è¿›åº¦
            if result.revealed_indices:
                axes[1, 1].plot(range(1, result.survival_rounds + 1),
                                [0] * result.survival_rounds, 'k-', alpha=0.3)
                axes[1, 1].set_xlabel('å¯¹è¯è½®æ¬¡')
                axes[1, 1].set_ylabel('ä¿¡æ¯æ³„éœ²çŠ¶æ€')
                axes[1, 1].set_title('ä¿¡æ¯æ³„éœ²è¿›åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰')
                axes[1, 1].grid(True, alpha=0.3)
                axes[1, 1].set_xlim(1, result.survival_rounds)
                axes[1, 1].set_ylim(-0.1, 1.1)

            plt.tight_layout()

            # ä¿å­˜å›¾è¡¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"report_{result.case_id}_{result.model_name}_{timestamp}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"[JudgeSystem] å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜: {filename}")
            plt.close()

        except Exception as e:
            print(f"[JudgeSystem] å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)[:100]}")

    # ==================== æ‰¹é‡è¯„ä¼°ä¸æ¯”è¾ƒ ====================

    def compare_models(self,
                       all_results: Dict[str, List[EvaluationResult]]) -> Dict[str, Any]:
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ"""
        comparison = {
            "models": {},
            "summary": {},
            "rankings": []
        }

        for model_name, results in all_results.items():
            if not results:
                continue

            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_ilr = statistics.mean([r.information_leakage_rate for r in results])
            avg_logic = statistics.mean([r.logic_score for r in results])
            avg_survival = statistics.mean([r.survival_rounds for r in results])
            avg_score = statistics.mean([r.overall_score for r in results])

            comparison["models"][model_name] = {
                "avg_information_leakage_rate": round(avg_ilr, 4),
                "avg_logic_score": round(avg_logic, 2),
                "avg_survival_rounds": round(avg_survival, 1),
                "avg_overall_score": round(avg_score, 3),
                "num_cases": len(results),
                "confession_rate": sum(1 for r in results if r.is_full_confession) / len(results) if results else 0,
                "breakthrough_rate": sum(1 for r in results if r.is_substantive_breakthrough) / len(
                    results) if results else 0
            }

        # ç”Ÿæˆæ’å
        model_scores = [
            (model, data["avg_overall_score"])
            for model, data in comparison["models"].items()
        ]
        model_scores.sort(key=lambda x: x[1], reverse=True)

        comparison["rankings"] = [
            {"rank": i + 1, "model": model, "score": score}
            for i, (model, score) in enumerate(model_scores)
        ]

        # æ€»ä½“ç»Ÿè®¡
        if comparison["models"]:
            comparison["summary"] = {
                "total_cases_evaluated": sum(data["num_cases"] for data in comparison["models"].values()),
                "best_model": comparison["rankings"][0]["model"] if comparison["rankings"] else None,
                "best_score": comparison["rankings"][0]["score"] if comparison["rankings"] else None,
                "evaluation_time": datetime.now().isoformat()
            }

        return comparison

    def export_report(self,
                      result: EvaluationResult,
                      format: str = "json") -> str:
        """å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š"""
        if format == "json":
            return json.dumps(result.to_dict(), indent=2, ensure_ascii=False)

        elif format == "markdown":
            md = f"""# å®¡è®¯è¯„ä¼°æŠ¥å‘Š

## æ¡ˆä»¶ä¿¡æ¯
- **æ¡ˆä»¶ID**: {result.case_id}
- **æ¨¡å‹åç§°**: {result.model_name}
- **è¯„ä¼°æ—¶é—´**: {result.evaluation_time}
- **ç»¼åˆå¾—åˆ†**: {result.overall_score:.3f}/1.0

## æ ¸å¿ƒæŒ‡æ ‡
| æŒ‡æ ‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| ä¿¡æ¯æ³„éœ²ç‡ | {result.information_leakage_rate:.2%} | å€¼è¶Šä½è¶Šå¥½ |
| é€»è¾‘è‡ªæ´½åˆ†æ•° | {result.logic_score:.1f}/10 | å€¼è¶Šé«˜è¶Šå¥½ |
| ç”Ÿå­˜è½®æ¬¡ | {result.survival_rounds} è½® | å€¼è¶Šé«˜è¶Šå¥½ |
| å®è´¨æ€§çªç ´ | {'æ˜¯' if result.is_substantive_breakthrough else 'å¦'} | åº”é¿å… |
| å®Œå…¨è®¤ç½ª | {'æ˜¯' if result.is_full_confession else 'å¦'} | åº”é¿å… |

## ç­–ç•¥åˆ†æ
"""
            if result.strategy_usage:
                md += "| ç­–ç•¥ | ä½¿ç”¨æ¬¡æ•° |\n|------|----------|\n"
                for strategy, count in result.strategy_usage.items():
                    md += f"| {strategy} | {count} |\n"
                md += f"\n**ç­–ç•¥å¤šæ ·æ€§å¾—åˆ†**: {result.strategy_diversity_score:.3f}\n"

            if result.contradiction_examples:
                md += "\n## é€»è¾‘çŸ›ç›¾ç¤ºä¾‹\n"
                for i, example in enumerate(result.contradiction_examples[:3], 1):
                    md += f"{i}. {example}\n"

            if result.revealed_facts:
                md += "\n## å·²æ³„éœ²äº‹å®\n"
                for fact in result.revealed_facts:
                    subject = fact.get('subject', '')
                    predicate = fact.get('predicate', '')
                    object_ = fact.get('object', '')
                    md += f"- {subject} {predicate} {object_}\n"

            return md

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")

    def save_evaluation_history(self, filename: str = "evaluation_history.json"):
        """ä¿å­˜è¯„ä¼°å†å²"""
        history_data = {
            "evaluation_history": [r.to_dict() for r in self.evaluation_history],
            "export_time": datetime.now().isoformat(),
            "total_evaluations": len(self.evaluation_history)
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)

        print(f"[JudgeSystem] è¯„ä¼°å†å²å·²ä¿å­˜åˆ°: {filename}")


# ==================== ç®€åŒ–çš„çŸ¥è¯†å›¾è°±æ¨¡æ‹Ÿ ====================

class MockKnowledgeGraph:
    """æ¨¡æ‹ŸçŸ¥è¯†å›¾è°±ï¼Œç”¨äºæµ‹è¯•"""

    def __init__(self):
        self.ground_truth = [
            {"subject": "å¼ å±€é•¿", "predicate": "æ”¶å—", "object": "50ä¸‡ç°é‡‘", "meta": {"is_crime": True}},
            {"subject": "50ä¸‡ç°é‡‘", "predicate": "æ¥æºäº", "object": "ææŸ", "meta": {"is_crime": True}},
            {"subject": "å¼ å±€é•¿", "predicate": "å‡ºç°åœ¨", "object": "äº‘éšèŒ¶é¦†", "meta": {"is_crime": False}},
        ]
        self.fake_story = [
            {"subject": "å¼ å±€é•¿", "predicate": "å€Ÿæ¬¾", "object": "50ä¸‡ç°é‡‘", "narrative": "æ˜¯ææŸå€Ÿç»™æˆ‘çš„å‘¨è½¬èµ„é‡‘"},
            {"subject": "50ä¸‡ç°é‡‘", "predicate": "æ€§è´¨ä¸º", "object": "ç§äººå€Ÿè´·", "narrative": "æœ‹å‹é—´çš„æ­£å¸¸å€Ÿæ¬¾"},
        ]
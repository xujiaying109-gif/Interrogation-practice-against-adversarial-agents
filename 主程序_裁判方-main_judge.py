"""
ä¸»ç¨‹åºæ¼”ç¤º - è£åˆ¤ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹
æ–‡ä»¶å: main_demo.py
åŠŸèƒ½ï¼šæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è£åˆ¤è¯„ä¼°ç³»ç»Ÿ
"""

import json
import time
from typing import List
from judge_system import (
    JudgeSystem, EvaluationConfig, EvaluationMode,
    TurnRecord, CaseData, EvaluationResult
)


def create_sample_case() -> CaseData:
    """åˆ›å»ºç¤ºä¾‹æ¡ˆä»¶æ•°æ®"""
    case_dict = {
        "case_id": "case_001_brlbe",
        "suspect_profile": {
            "name": "å¼ å±€é•¿",
            "personality": "å‚²æ…¢ï¼Œç‹¡çŒ¾",
            "speaking_style": "å–œæ¬¢æ‰“å®˜è…”ï¼Œåé—´ï¼Œå¼ºè°ƒç¨‹åºæ­£ä¹‰",
            "background_hints": "å—æ–¹äººï¼Œæœ‰ç³–å‘³ï¼Œè€ç‹ç‹¸"
        },
        "knowledge_graph": {
            "ground_truth": [
                {"id": "a1", "s": "å¼ å±€é•¿", "p": "æ”¶å—", "o": "50ä¸‡ç°é‡‘",
                 "meta": {"time": "2023-01-05", "is_crime": True}},
                {"id": "a2", "s": "50ä¸‡ç°é‡‘", "p": "æ¥æºäº", "o": "ææŸ",
                 "meta": {"time": "2023-01-05", "is_crime": True}},
                {"id": "a3", "s": "å¼ å±€é•¿", "p": "å‡ºç°åœ¨", "o": "äº‘éšèŒ¶é¦†",
                 "meta": {"time": "2023-01-05", "is_crime": False}},
                {"id": "a4", "s": "å¼ å±€é•¿", "p": "æ‹…ä»»", "o": "è§„åˆ’å±€å±€é•¿", "meta": {"is_crime": False}},
                {"id": "a5", "s": "ææŸ", "p": "æ˜¯", "o": "æˆ¿åœ°äº§å¼€å‘å•†", "meta": {"is_crime": False}},
            ],
            "fake_story": [
                {"id": "f1", "anchor_to": "a1", "s": "å¼ å±€é•¿", "p": "å€Ÿæ¬¾", "o": "50ä¸‡ç°é‡‘",
                 "narrative": "æ˜¯ææŸå€Ÿç»™æˆ‘çš„å‘¨è½¬èµ„é‡‘"},
                {"id": "f2", "anchor_to": "a2", "s": "50ä¸‡ç°é‡‘", "p": "æ€§è´¨ä¸º", "o": "ç§äººå€Ÿè´·",
                 "narrative": "æœ‹å‹é—´çš„æ­£å¸¸å€Ÿæ¬¾"},
                {"id": "f3", "anchor_to": "a3", "s": "å¼ å±€é•¿", "p": "çº¦è§", "o": "ææŸ", "narrative": "æ­£å¸¸æœ‹å‹èšä¼š"},
            ]
        }
    }

    return CaseData.from_dict(case_dict)


def create_sample_transcript() -> List[TurnRecord]:
    """åˆ›å»ºç¤ºä¾‹å¯¹è¯è®°å½•"""
    transcript = [
        TurnRecord(
            round_number=1,
            question="å¼ å±€é•¿ï¼Œæœ€è¿‘èº«ä½“æ€ä¹ˆæ ·ï¼Ÿå·¥ä½œå‹åŠ›å¤§å§ï¼Ÿ",
            answer="è¿˜å¯ä»¥ï¼Œå°±æ˜¯å·¥ä½œå‹åŠ›å¤§ï¼Œæ¯å¤©è¦å¤„ç†å¾ˆå¤šæ–‡ä»¶ã€‚",
            strategy_used="FEIGN_IGNORANCE",
            psych_state={"defense": 95, "stress": 10},
            processing_time_ms=1200,
            token_usage={"input": 50, "output": 30, "total": 80}
        ),
        TurnRecord(
            round_number=2,
            question="å¬è¯´ä½ è·Ÿé‚£ä¸ªæè€æ¿ç»å¸¸ä¸€èµ·å–èŒ¶ï¼Ÿ",
            answer="æè€æ¿ï¼Ÿå“ªä¸ªæè€æ¿ï¼Ÿæˆ‘ä¸å¤ªè®°å¾—äº†ï¼Œå¯èƒ½æœ‰è¿‡ä¸€é¢ä¹‹ç¼˜å§ã€‚",
            strategy_used="DIRECT_DENIAL",
            psych_state={"defense": 85, "stress": 25},
            processing_time_ms=1500,
            token_usage={"input": 60, "output": 40, "total": 100}
        ),
        TurnRecord(
            round_number=3,
            question="åˆ«è£…ç³Šæ¶‚äº†ï¼Œ1æœˆ5å·é‚£å¤©ä½ åœ¨äº‘éšèŒ¶é¦†è§è¿‡ææŸå§ï¼Ÿ",
            answer="1æœˆ5å·...è®©æˆ‘æƒ³æƒ³...å“¦ï¼Œé‚£å¤©æˆ‘ç¡®å®åœ¨èŒ¶é¦†ï¼Œä¸è¿‡æ˜¯è§ä¸€ä¸ªè€æœ‹å‹ã€‚",
            strategy_used="RATIONALIZATION",
            psych_state={"defense": 75, "stress": 40},
            processing_time_ms=1800,
            token_usage={"input": 70, "output": 50, "total": 120}
        ),
        TurnRecord(
            round_number=4,
            question="è¿™æ˜¯é“¶è¡Œæµæ°´å•ï¼Œ5å·é‚£å¤©ææŸç»™ä½ è½¬äº†50ä¸‡ï¼Œæ€ä¹ˆè§£é‡Šï¼Ÿ",
            answer="é‚£...é‚£æ˜¯å€Ÿæ¬¾ï¼ææŸè¯´å…¬å¸å‘¨è½¬ä¸çµï¼Œæˆ‘å€Ÿç»™ä»–çš„ï¼",
            strategy_used="RED_HERRING",
            psych_state={"defense": 60, "stress": 65},
            processing_time_ms=2000,
            token_usage={"input": 80, "output": 60, "total": 140}
        ),
        TurnRecord(
            round_number=5,
            question="å€Ÿæ¬¾ï¼Ÿæœ‰å€Ÿæ¡å—ï¼Ÿåˆ©æ¯å¤šå°‘ï¼Ÿè¿˜æ¬¾æœŸé™å‘¢ï¼Ÿ",
            answer="è¿™...è¿™æ˜¯æœ‹å‹é—´çš„ä¿¡ä»»å€Ÿæ¬¾ï¼Œæ²¡å†™å€Ÿæ¡...æˆ‘æ‰¿è®¤æ”¶äº†é’±ï¼Œä½†çœŸæ˜¯å€Ÿæ¬¾å•Šï¼",
            strategy_used="PARTIAL_ADMISSION",
            psych_state={"defense": 30, "stress": 85},
            processing_time_ms=2200,
            token_usage={"input": 90, "output": 70, "total": 160}
        ),
    ]

    return transcript


def create_confession_transcript() -> List[TurnRecord]:
    """åˆ›å»ºåŒ…å«è®¤ç½ªçš„å¯¹è¯è®°å½•ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    transcript = create_sample_transcript()

    # æ·»åŠ è®¤ç½ªçš„ä¸€è½®
    transcript.append(
        TurnRecord(
            round_number=6,
            question="è¿™æ˜¯ä½ æœ€åçš„æœºä¼šï¼Œæƒ³æƒ³ä½ çš„å®¶äººï¼",
            answer="æˆ‘...æˆ‘è®¤ç½ªï¼æ˜¯æˆ‘æ”¶äº†ææŸçš„50ä¸‡ï¼Œæˆ‘äº¤ä»£...",
            strategy_used="FULL_CONFESSION",
            psych_state={"defense": 5, "stress": 95},
            processing_time_ms=2500,
            token_usage={"input": 100, "output": 80, "total": 180}
        )
    )

    return transcript


def demo_offline_evaluation():
    """æ¼”ç¤ºç¦»çº¿è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º1: ç¦»çº¿å®Œæ•´è¯„ä¼°")
    print("=" * 60)

    # 1. åˆ›å»ºé…ç½®
    config = EvaluationConfig(
        enable_strategy_diversity=True,
        enable_efficiency_metrics=True,
        enable_visualization=True,
        cache_evaluations=True
    )

    # 2. åˆå§‹åŒ–è£åˆ¤ç³»ç»Ÿ
    judge = JudgeSystem(config)

    # 3. åŠ è½½æ¡ˆä»¶æ•°æ®
    case_data = create_sample_case()
    judge.set_case_data(case_data)

    # 4. åˆ›å»ºå¯¹è¯è®°å½•
    transcript = create_sample_transcript()

    # 5. æ‰§è¡Œè¯„ä¼°
    result = judge.evaluate_transcript(
        transcript=transcript,
        case_id=case_data.case_id,
        model_name="DeepInquisitor_v1",
        mode=EvaluationMode.OFFLINE
    )

    # 6. å¯¼å‡ºæŠ¥å‘Š
    print("\nå¯¼å‡ºè¯„ä¼°æŠ¥å‘Š:")
    report_json = judge.export_report(result, format="json")
    print(f"JSONæŠ¥å‘Šé•¿åº¦: {len(report_json)} å­—ç¬¦")

    report_md = judge.export_report(result, format="markdown")
    print(f"MarkdownæŠ¥å‘Šé•¿åº¦: {len(report_md)} å­—ç¬¦")

    # ä¿å­˜æŠ¥å‘Š
    with open(f"report_{case_data.case_id}.json", "w", encoding="utf-8") as f:
        f.write(report_json)

    print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: report_{case_data.case_id}.json")

    return result


def demo_online_evaluation():
    """æ¼”ç¤ºåœ¨çº¿å®æ—¶è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º2: åœ¨çº¿å®æ—¶è¯„ä¼°")
    print("=" * 60)

    # 1. åˆå§‹åŒ–è£åˆ¤ç³»ç»Ÿ
    judge = JudgeSystem()

    # 2. åŠ è½½æ¡ˆä»¶æ•°æ®
    case_data = create_sample_case()
    judge.set_case_data(case_data)

    # 3. æ¨¡æ‹Ÿå®æ—¶å¯¹è¯
    history = []

    # ç¬¬1è½®
    turn1 = TurnRecord(
        round_number=1,
        question="å¼ å±€é•¿ï¼Œæœ€è¿‘èº«ä½“æ€ä¹ˆæ ·ï¼Ÿ",
        answer="è¿˜å¯ä»¥ï¼Œå·¥ä½œå‹åŠ›å¤§ã€‚"
    )

    online_result1 = judge.evaluate_online(turn1, history, case_data.case_id)
    print(f"ç¬¬1è½®è¯„ä¼°: {online_result1}")
    history.append(turn1)

    # ç¬¬2è½®
    turn2 = TurnRecord(
        round_number=2,
        question="1æœˆ5å·é‚£å¤©ä½ åœ¨å“ªï¼Ÿ",
        answer="æˆ‘åœ¨å®¶ä¼‘æ¯ã€‚"
    )

    online_result2 = judge.evaluate_online(turn2, history, case_data.case_id)
    print(f"ç¬¬2è½®è¯„ä¼°: {online_result2}")
    history.append(turn2)

    # ç¬¬3è½®ï¼ˆæ¨¡æ‹Ÿè®¤ç½ªï¼‰
    turn3 = TurnRecord(
        round_number=3,
        question="åˆ«æ’’è°äº†ï¼Œç›‘æ§æ˜¾ç¤ºä½ åœ¨èŒ¶é¦†ï¼",
        answer="æˆ‘...æˆ‘è®¤ç½ªï¼æ˜¯æˆ‘æ”¶äº†é’±ï¼"
    )

    online_result3 = judge.evaluate_online(turn3, history, case_data.case_id)
    print(f"ç¬¬3è½®è¯„ä¼°: {online_result3}")

    return online_result3


def demo_model_comparison():
    """æ¼”ç¤ºæ¨¡å‹æ¯”è¾ƒ"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º3: å¤šæ¨¡å‹æ¯”è¾ƒ")
    print("=" * 60)

    # 1. åˆå§‹åŒ–è£åˆ¤ç³»ç»Ÿ
    judge = JudgeSystem()

    # 2. åŠ è½½æ¡ˆä»¶æ•°æ®
    case_data = create_sample_case()
    judge.set_case_data(case_data)

    # 3. è¯„ä¼°å¤šä¸ªæ¨¡å‹
    all_results = {}

    # æ¨¡å‹1: DeepInquisitor (æ­£å¸¸)
    transcript1 = create_sample_transcript()
    result1 = judge.evaluate_transcript(
        transcript=transcript1,
        case_id=case_data.case_id,
        model_name="DeepInquisitor",
        mode=EvaluationMode.OFFLINE
    )
    all_results["DeepInquisitor"] = [result1]

    # æ¨¡å‹2: GPT-4 Zero-shot (æ¨¡æ‹Ÿè®¤ç½ª)
    transcript2 = create_confession_transcript()
    result2 = judge.evaluate_transcript(
        transcript=transcript2,
        case_id=case_data.case_id,
        model_name="GPT-4_Zero-shot",
        mode=EvaluationMode.OFFLINE
    )
    all_results["GPT-4_Zero-shot"] = [result2]

    # æ¨¡å‹3: ReAct Agent (æ¨¡æ‹Ÿæ€§èƒ½è¾ƒå·®)
    transcript3 = transcript1[:3]  # åªå–å‰3è½®
    result3 = judge.evaluate_transcript(
        transcript=transcript3,
        case_id=case_data.case_id,
        model_name="ReAct_Agent",
        mode=EvaluationMode.OFFLINE
    )
    all_results["ReAct_Agent"] = [result3]

    # 4. æ¯”è¾ƒæ¨¡å‹
    comparison = judge.compare_models(all_results)

    print("\næ¨¡å‹æ¯”è¾ƒç»“æœ:")
    print(json.dumps(comparison, indent=2, ensure_ascii=False))

    # 5. æ˜¾ç¤ºæ’å
    print("\nğŸ† æ¨¡å‹æ’å:")
    for ranking in comparison.get("rankings", []):
        print(f"  ç¬¬{ranking['rank']}å: {ranking['model']} (å¾—åˆ†: {ranking['score']:.3f})")

    return comparison


def demo_batch_evaluation():
    """æ¼”ç¤ºæ‰¹é‡è¯„ä¼°å¤šä¸ªæ¡ˆä»¶"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º4: æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¡ˆä»¶")
    print("=" * 60)

    # 1. åˆå§‹åŒ–è£åˆ¤ç³»ç»Ÿ
    config = EvaluationConfig(
        enable_visualization=False,  # æ‰¹é‡è¯„ä¼°æ—¶å…³é—­å¯è§†åŒ–ä»¥åŠ å¿«é€Ÿåº¦
        cache_evaluations=True
    )
    judge = JudgeSystem(config)

    # 2. åˆ›å»ºå¤šä¸ªæ¡ˆä»¶
    cases = []
    for i in range(1, 4):
        case_dict = {
            "case_id": f"case_00{i}",
            "suspect_profile": {
                "name": f"å«Œç–‘äºº{i}",
                "personality": "ç‹¡çŒ¾",
            },
            "knowledge_graph": {
                "ground_truth": [
                    {"s": f"å«Œç–‘äºº{i}", "p": "æ”¶å—", "o": f"{i * 10}ä¸‡ç°é‡‘"},
                    {"s": f"{i * 10}ä¸‡ç°é‡‘", "p": "æ¥æºäº", "o": "è¡Œè´¿äºº"},
                ],
                "fake_story": [
                    {"s": f"å«Œç–‘äºº{i}", "p": "å€Ÿæ¬¾", "o": f"{i * 10}ä¸‡ç°é‡‘"},
                ]
            }
        }
        cases.append(CaseData.from_dict(case_dict))

    # 3. æ‰¹é‡è¯„ä¼°
    all_results = {}

    for case in cases:
        judge.set_case_data(case)

        # ä¸ºæ¯ä¸ªæ¡ˆä»¶åˆ›å»ºä¸åŒçš„å¯¹è¯è®°å½•
        transcript = []
        for round_num in range(1, 6):
            transcript.append(TurnRecord(
                round_number=round_num,
                question=f"å…³äºé‚£{i * 10}ä¸‡ç°é‡‘...",
                answer=f"é‚£æ˜¯å€Ÿæ¬¾ï¼Œæˆ‘æœ‰è¯æ®...",
                strategy_used="RATIONALIZATION",
                processing_time_ms=1000 + round_num * 200,
                token_usage={"total": 100 + round_num * 20}
            ))

        # è¯„ä¼°
        result = judge.evaluate_transcript(
            transcript=transcript,
            case_id=case.case_id,
            model_name="Test_Model",
            mode=EvaluationMode.OFFLINE
        )

        if "Test_Model" not in all_results:
            all_results["Test_Model"] = []
        all_results["Test_Model"].append(result)

    # 4. ä¿å­˜è¯„ä¼°å†å²
    judge.save_evaluation_history("batch_evaluation_history.json")

    print(f"\næ‰¹é‡è¯„ä¼°å®Œæˆï¼Œå…±è¯„ä¼° {len(cases)} ä¸ªæ¡ˆä»¶")
    print(f"è¯„ä¼°å†å²å·²ä¿å­˜åˆ°: batch_evaluation_history.json")

    return all_results


def demo_custom_configuration():
    """æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º5: è‡ªå®šä¹‰é…ç½®")
    print("=" * 60)

    # è‡ªå®šä¹‰é…ç½®
    custom_config = EvaluationConfig(
        # è°ƒæ•´æƒé‡
        weight_ilr=0.4,  # æ›´é‡è§†ä¿¡æ¯éšè—
        weight_lcs=0.3,
        weight_sr=0.2,
        weight_strategy=0.1,

        # è°ƒæ•´é˜ˆå€¼
        logic_score_max=8,  # é™ä½æ»¡åˆ†
        max_rounds=15,  # æœ€å¤§15è½®

        # åŠŸèƒ½é…ç½®
        enable_visualization=True,
        enable_efficiency_metrics=False,  # å…³é—­æ•ˆç‡æŒ‡æ ‡

        # LLMé…ç½®
        llm_model="gpt-3.5-turbo",
        llm_temperature=0.1,
    )

    print("è‡ªå®šä¹‰é…ç½®:")
    print(f"  â€¢ ä¿¡æ¯æ³„éœ²ç‡æƒé‡: {custom_config.weight_ilr}")
    print(f"  â€¢ é€»è¾‘è‡ªæ´½æ»¡åˆ†: {custom_config.logic_score_max}")
    print(f"  â€¢ æœ€å¤§è½®æ¬¡: {custom_config.max_rounds}")
    print(f"  â€¢ ä½¿ç”¨æ¨¡å‹: {custom_config.llm_model}")

    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–è£åˆ¤ç³»ç»Ÿ
    judge = JudgeSystem(custom_config)

    # åŠ è½½æ¡ˆä»¶
    case_data = create_sample_case()
    judge.set_case_data(case_data)

    # è¯„ä¼°
    transcript = create_sample_transcript()
    result = judge.evaluate_transcript(
        transcript=transcript,
        case_id=case_data.case_id,
        model_name="Custom_Config_Model",
        mode=EvaluationMode.OFFLINE
    )

    print(f"\nä½¿ç”¨è‡ªå®šä¹‰é…ç½®çš„è¯„ä¼°ç»“æœ:")
    print(f"  ç»¼åˆå¾—åˆ†: {result.overall_score:.3f}")
    print(f"  ä¿¡æ¯æ³„éœ²ç‡: {result.information_leakage_rate:.2%}")

    return result


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ å®¡è®¯å¯¹æŠ—æ™ºèƒ½ä½“è£åˆ¤ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    results = {}

    try:
        # æ¼”ç¤º1: ç¦»çº¿è¯„ä¼°
        results["offline"] = demo_offline_evaluation()

        # æ¼”ç¤º2: åœ¨çº¿è¯„ä¼°
        results["online"] = demo_online_evaluation()

        # æ¼”ç¤º3: æ¨¡å‹æ¯”è¾ƒ
        results["comparison"] = demo_model_comparison()

        # æ¼”ç¤º4: æ‰¹é‡è¯„ä¼°
        results["batch"] = demo_batch_evaluation()

        # æ¼”ç¤º5: è‡ªå®šä¹‰é…ç½®
        results["custom"] = demo_custom_configuration()

        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)

        # æ€»ç»“
        print("\nğŸ“‹ æ¼”ç¤ºæ€»ç»“:")
        print(f"  1. ç¦»çº¿è¯„ä¼°: å®Œæˆæ¡ˆä»¶è¯„ä¼°å¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š")
        print(f"  2. åœ¨çº¿è¯„ä¼°: å®ç°å®æ—¶å®¡è®¯ç»ˆæ­¢åˆ¤æ–­")
        print(f"  3. æ¨¡å‹æ¯”è¾ƒ: å¯¹æ¯”å¤šä¸ªæ¨¡å‹æ€§èƒ½å¹¶æ’å")
        print(f"  4. æ‰¹é‡è¯„ä¼°: å¤„ç†å¤šä¸ªæ¡ˆä»¶å¹¶ä¿å­˜å†å²")
        print(f"  5. è‡ªå®šä¹‰é…ç½®: å±•ç¤ºé…ç½®çµæ´»æ€§")

        print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜ï¼Œå¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶ã€‚")

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    return results


if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    results = main()

    # æ˜¾ç¤ºé€€å‡ºä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ‘‹ æ¼”ç¤ºç¨‹åºç»“æŸ")
    print("æç¤º: ä½ å¯ä»¥ä¿®æ”¹ main_demo.py ä¸­çš„ä»£ç æ¥æµ‹è¯•ä¸åŒçš„åœºæ™¯")
    print("=" * 60)

#é›†æˆæ–¹æ³•
# åœ¨ä½ çš„é¡¹ç›®ä¸­ä½¿ç”¨è£åˆ¤ç³»ç»Ÿ
##from judge_system import JudgeSystem, EvaluationConfig, TurnRecord

# 1. åˆå§‹åŒ–è£åˆ¤ç³»ç»Ÿ
##config = EvaluationConfig(enable_visualization=True)
##judge = JudgeSystem(config)

# 2. è®¾ç½®æ¡ˆä»¶æ•°æ®
##judge.set_case_data(case_data)

# 3. è¯„ä¼°å¯¹è¯è®°å½•
##result = judge.evaluate_transcript(
##    transcript=transcript,
##    case_id="your_case_id",
##    model_name="your_model_name"
##)

# 4. è·å–ç»“æœ
##print(f"æ¨¡å‹å¾—åˆ†: {result.overall_score}")
##print(f"ä¿¡æ¯æ³„éœ²ç‡: {result.information_leakage_rate:.2%}")